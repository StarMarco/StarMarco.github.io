<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.310">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Star">
<meta name="dcterms.date" content="2023-04-19">

<title>Star Research Blog - Kalman Filters and Generative Modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Star Research Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Kalman Filters and Generative Modelling</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Marco Star </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 19, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<blockquote class="blockquote">
<p>Kalman filters are a type of Bayesian filter (at least that is the lens we will use to analyze them). They solve a type of mathematical problem known as the filtering problem analytically by assuming we have a linear Gaussian state-space system. This means the relationships between our states and measurements are linear and the states and measurements are assumed to be Normally distributed. Despite these restrictive assumptions Kalman filters are widely used for applications such as object tracking (e.g.&nbsp;missiles, robots, vehicles etc.) and navigation. Even if that wasn’t the case it is fascinating to explore the topic of filtering through the Kalman filter due to how (relatively) accessible it is. In this post I want to go over some of the theory behind the Kalman filter and how Bayesian filters can be used to train generative models. <em>Note, I assume in this post you know about VAEs or DVAEs, so if you don’t, go read my previous posts on those topics or look them up online</em></p>
</blockquote>
<section id="objective-of-this-post" class="level1">
<h1>Objective of this Post</h1>
<p>I like generative models. I’m even strange in the sense the applications aren’t what I like the most (as cool as some of them are), it’s the underlying theory. This is a fun post for me because control theory and Kalman filtering is what got me into research to begin with. I love learning about these topics in particular, there is something about the theory I find satisfying and interesting. When learning about Variational Autoencoders (VAEs) and Dynamical Variational Autoencoders (DVAEs), the similarities between these models and Bayesian filtering started to become clearer the more I studied them. Finally, when I learned about using Importance Sampling to improve the objective function for training VAEs notably in the sequential case outlined by papers such as <span class="citation" data-cites="Naesseth2017">(<a href="#ref-Naesseth2017" role="doc-biblioref">Naesseth et al. 2017</a>)</span> and <span class="citation" data-cites="Maddison2017">(<a href="#ref-Maddison2017" role="doc-biblioref">Maddison et al. 2017</a>)</span>, the connection became clear. I wanted to share some these connections in this post as it may clarify the theory behind generative models as it did for me. While the papers I cited use a Particle Filter framework I will use the Kalman filter as an example to make this easier to understand.</p>
<p>Hence, in this post I will first go over some of the theory behind Kalman filtering and then talk about how they related to generative models. Let’s get started.</p>
</section>
<section id="kalman-filtering" class="level1">
<h1>Kalman Filtering</h1>
<section id="basic-concept" class="level2">
<h2 class="anchored" data-anchor-id="basic-concept">Basic Concept</h2>
<p>Imagine we have a tank of liquid, and we want to have an accurate reading of its temperature (perhaps we want to control this variable and keep it at a set value, but that isn’t important here). “Okay, let’s put a sensor that measures the temperature” you say. To which I would reply “good, but can we do better”. Note that the sensors have noise or a certain tolerance, and so we may want to “filter” out this noise and have a tighter tolerance. How could we do this? The Kalman filters answer would be to say that we have more information than just the sensor readings. We have the physics of the system, more precisely we have a mathematical model that can describe how the temperature should change as we add or take away heat from the system. So we can calculate what the temperature should be using the mathematical model, and we have a sensor reading. The Kalman filter is a tool that takes both of these and finds a new distribution that describes an updated temperature distribution which accounts for both the mathematical model’s estimated value and the sensor reading. In a way the Kalman filter is just a fancy way of taking a weighted average between the estimated probability distribution of our mathematical model and the estimated distribution of our sensor value at the current time. The example below shows two Gaussian distributions multiplied to find a new distribution which is essentially a weighted average of the two product Gaussian’s.</p>
<div class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<p><img src="Kalman_files/figure-html/cell-2-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>A few things can be noticed here. One this betrays an important property that is vital to the Kalman filter working. The product of 2 Gaussian distributions is also a Gaussian distribution. Hence, as long as our variables are Normally distributed the Kalman Filtering algorithm will work as it only needs to worry about Gaussian distributions. This is also important because the Gaussian distribution is fully described by the mean and variance of the distribution. Hence, when we are updating to the resultant filtered distribution, we only need to worry about 2 variables, the mean and variance (covariance in the multivariate case). That’s essentially what the Kalman filter is, a set of equations that describe how to propagate the mean and variance forward in time and another set of equations to update the state’s mean and variance based on the mean and variance of the sensor reading (or any observation related to the state). The other important assumption is that these equations have to be linear. This is important for the same reason, if they were nonlinear equations, the distribution would no longer be Gaussian distributions and so the equations would no longer apply.</p>
<p>Note that other Bayesian filters like the Kalman filter apply the same concept that I explained here. However, they may use non-Gaussian distributions or nonlinear dynamics and so the Kalman filter can no longer be used. Since the Kalman filter is a simple case I am using it in this post to illustrate certain concepts; however, these ideas can be extended to more general filters like the Particle filter.</p>
</section>
<section id="algorithm" class="level2">
<h2 class="anchored" data-anchor-id="algorithm">Algorithm</h2>
<p>Given the following state space equations for the system,</p>
<p><span class="math display">\[
x_t = F_t x_{t-1} + w_t, \ w_t \sim \mathcal{N}(0,Q_t)
\]</span> <span class="math display">\[
y_t = H_t x_t + v_t, \ v_t \sim \mathcal{N}(0,R_t)
\]</span> where, <span class="math inline">\(x_t\)</span> is the state (temperature in our example), <span class="math inline">\(y_t\)</span> is the measurement or observation, <span class="math inline">\(F_t\)</span> describes the dynamics of the state, <span class="math inline">\(H_t\)</span> describes the measurement function that converts states to measurements (e.g.&nbsp;if the measurements are represented by voltages in an electronic sensor <span class="math inline">\(H_t\)</span> converts the state from temperature to voltage), <span class="math inline">\(w_t\)</span> is some random noise added to our mathematical model known as the process noise, <span class="math inline">\(Q_t\)</span> is the covariance of that process noise, <span class="math inline">\(v_t\)</span> is the measurement noise and <span class="math inline">\(R_t\)</span> is the covariance that describes that noise.</p>
<p>The equations for the Kalman filter are as follows,</p>
<ol type="1">
<li>First propagate the previous time mean and covariance to the current time using our state space model. Also convert those variables to the measurement domain so we can directly compare our estimates to the observations. If they have different units for example we couldn’t fairly compare them, e.g.&nbsp;it would be like saying 1000mm is way bigger compared to 1m. <span class="math display">\[
\hat{x}^-_{t} = F_t \hat{x}^+_{t-1}
\]</span> <span class="math display">\[
P^-_t = F_t P^+_{t-1} F_t^T + Q_t
\]</span> <span class="math display">\[
\mu_t = H_t \hat{x}^-_t
\]</span> <span class="math display">\[
S_t = H_t P_t^- H_t^T + R_k
\]</span></li>
<li>Now incorporate the measurement (take the weighted average) <span class="math display">\[
K_t = P^-_t H_t^T S_k^{-1}
\]</span> <span class="math display">\[
\hat{x}^+_t = x^-_t + K_t(y_t - \mu_t)
\]</span> <span class="math display">\[
P^+_t = (I - K_t H_t)P^-_t
\]</span></li>
</ol>
<p>Here I have used the “hat” symbol to refer to the mean of the state i.e.&nbsp;<span class="math inline">\(\hat{x}\)</span> is the mean of the random variable <span class="math inline">\(x_t\)</span>. Also note, <span class="math inline">\(P_t\)</span> is the covariance of the state, the <span class="math inline">\(-\)</span> superscript refers to a “prior” and <span class="math inline">\(+\)</span> is the updated “posterior” statistic e.g.&nbsp;this shows us that <span class="math inline">\(P_t^-\)</span> is the prior covariance before we accounted for the measurement update, and once we do that we get <span class="math inline">\(P_t^+\)</span>. <span class="math inline">\(\mu\)</span> and <span class="math inline">\(S\)</span> are the mean and covariance of the state transfered to the measurement space.</p>
</section>
</section>
<section id="bayesian-formulation" class="level1">
<h1>Bayesian Formulation</h1>
<p>Since we are updating the distribution based on observations, this leads to a nice Bayesian interpretation of the filter. Because of this we can call the Kalman filter a Bayesian filter and other filters that solve the same filtering problem can fit into this category (e.g.&nbsp;Particle filter). Let’s first look at a general Bayesian formulation of a filter before we dive specifically into the Kalman filter.</p>
<p>We can write the Bayesian formulation of a filter as,</p>
<p><span class="math display">\[
p(x_t|y_{1:t}) = \frac{p(y_{1:t}|x_t)p(x_t)}{p(y_{1:t})}
\]</span></p>
<p>Notice we can split the sequence of observations from <span class="math inline">\(y_{1:t}\)</span> to <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{1:t-1}\)</span> i.e.</p>
<p><span class="math display">\[
p(x_t|y_{1:t}) = \frac{p(y_t, y_{1:t-1}|x_t)p(x_t)}{p(y_t, y_{1:t-1})}
\]</span> <span class="math display">\[
p(x_t|y_{1:t}) = \frac{p(y_t|x_t)p(y_{1:t-1}|x_t)p(x_t)}{p(y_t|y_{1:t-1})p(y_{1:t-1})}
\]</span> <span class="math display">\[
p(x_t|y_{1:t}) = \frac{p(y_t|x_t)}{p(y_t|y_{1:t-1})}\frac{p(y_{1:t-1}|x_t)p(x_t)}{p(y_{1:t-1})}
\]</span> <span class="math display">\[
p(x_t|y_{1:t}) = \frac{p(y_t|x_t)p(x_t|y_{1:t-1})}{p(y_t|y_{1:t-1})}
\]</span></p>
<p>Note that the denominator is essentially a normalization constant for the probability distribution <span class="math inline">\(p(x_t|y_{1:t})\)</span>, and so we can find it by summing the numerator. In the continuous case it would be an integral not a sum, so we get, <span class="math display">\[
p(x_t|y_{1:t}) = \frac{p(y_t|x_t)p(x_t|y_{1:t-1})}{\int p(y_t|x_t)p(x_t|y_{1:t-1}) dx_t}
\]</span> When I said we could find the normalizing constant by computing the integral <span class="math inline">\(\int p(y_t|x_t)p(x_t|y_{1:t-1}) dx_t\)</span> I lied. Well, kind of, in most cases it is really difficult to solve this integral although not in the Kalman filtering case which is why it is the topic of this post.</p>
<p>The distribution <span class="math inline">\(p(y_{1:t})\)</span> is known as the marginal likelihood. From our Bayesian Filtering formulation you can see we can compute the marginal likelihood recursively using, <span class="math display">\[
p(y_{1:t}) = \prod_{k=1}^t p(y_k|y_{1:k-1})
\]</span> Note, when <span class="math inline">\(k=1\)</span> and <span class="math inline">\(p(y_1|y_{1:0})\)</span> we will just take this to mean <span class="math inline">\(p(y_1)\)</span>. We would want to compute the marginal likelihood because it is a good loss function, i.e.&nbsp;maximising this likelihood would maximise the likelihood our model captures the actual observations, which is what we want. However, it is difficult to caculate it because as I mentioned <span class="math inline">\(p(y_t|y_{1:t-1}) = \int p(y_t|x_t)p(x_t|y_{1:t-1}) dx_t\)</span> which is a difficult integral to find. Although for the Kalman filter finding <span class="math inline">\(p(y_t|y_{1:t-1})\)</span> isn’t too difficult. Note that <span class="math inline">\(\int p(y_t|x_t)p(x_t|y_{1:t-1}) dx_t\)</span> involves two distributions,</p>
<ul>
<li><span class="math inline">\(p(x_t|y_{1:t-1}) = \int p(x_t|x_{t-1}, y_{1:t-1}) p(x_{t-1}| y_{1:t-1})dx_{t-1}\)</span>, which essentially states using the previous filtering distribution <span class="math inline">\(p(x_{t-1}, y_{1:t-1})\)</span> and uses the dynamics <span class="math inline">\(p(x_t|x_{t-1}, y_{1:t-1})\)</span> defined by our state space model <span class="math inline">\(x^-_t = F_t x^+_{t-1} + w_t\)</span> to propagate that state foward in time.</li>
<li><span class="math inline">\(p(y_t|x_t)\)</span> simply converts our predicted state to the measurement/observation domain also defined by our state space model, <span class="math inline">\(y_t = H_t x_t^- + v_t\)</span>.</li>
</ul>
<p>Hence, for the Kalman filter these are all Gaussian distributions so we can find the mean and covariance of the propagated state using, <span class="math display">\[
\hat{x}_t^- = F_t \hat{x}_{t-1}^+
\]</span> <span class="math display">\[
P_t^- = F_t P_{t-1}^+ F_t^T + Q_t
\]</span> Then convert to the measurement domain <span class="math display">\[
\mu_t = H_t \hat{x}_t^-
\]</span> <span class="math display">\[
S_t = H_t P_t^- H_t^T + R_t
\]</span> Now <span class="math inline">\(p(y_t|y_{1:t-1}) = \mathcal{N}(\mu_t, S_t)\)</span> and the likelihood can be calculated by plugging in our observation <span class="math inline">\(y_t\)</span> into the Gaussian distribution. The idea would be the same for other Bayesian filters but most don’t have analytical solutions like the Kalman filter does. For example, the Particle filter could do this by propagating particles forward in time using the state dynamics and then using the measurement function. However, here the distribution would be representing by how ever many particles you choose to use and their weights.</p>
<p>After this all you would need is some optimization routine to optimize the parameters of your model based on this loss function. For example, the dynamics matrix <span class="math inline">\(F_t\)</span> could have some unknown elements in it denoted as <span class="math inline">\(\theta_t\)</span> that we might want to find to specify a model for our particular application. One might even want to use backprogation through some deep learning library like PyTorch or Tensorflow. This brings me to the connections betweens Variational Autoencoders (VAEs) and Bayesian filters.</p>
<p>Before I move on though, I should mention that if you are interested about some of the specifics on the Kalman filter and this Bayesian formulation of the Kalman filter (e.g.&nbsp;you might want how to actually derive the Kalman filtering equations using this general Bayes’ filter formulation) then I recommend a book by <span class="citation" data-cites="Sarkka2013">(<a href="#ref-Sarkka2013" role="doc-biblioref">Särkkä 2013</a>)</span> (which I think is free as a pdf download) or check out <a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python">this “book”</a> by Roger Labbe if you are a beginner to this. I say “book” because they are really a collection of jupyter notebooks to create a sort of interactive book where you can play around with the code. But both of these helped me learn about the Kalman filter.</p>
<section id="vaes-and-bayesian-filtering" class="level2">
<h2 class="anchored" data-anchor-id="vaes-and-bayesian-filtering">VAEs and Bayesian Filtering</h2>
<p>With a VAE we use Bayes’ theorem to learn a generative model that can generate samples from a data distribution <span class="math inline">\(p(Y)\)</span> (normally we use the variable <span class="math inline">\(X\)</span> but I didn’t want to confuse it with the states from above so since I used <span class="math inline">\(y_t\)</span> for measurements we’re sticking with that). We wanted to optimize the VAE by maximizing <span class="math inline">\(p(Y)\)</span>, or more practically, by minimizing <span class="math inline">\(-\text{log}p(Y)\)</span>. But we couldn’t because when using Bayes’ theorem we found,</p>
<p><span class="math display">\[
p(X|Y) = \frac{p(Y|X)p(X)}{p(Y)} = \frac{p(Y|X)p(X)}{\int p(Y|X)p(X)dX}
\]</span></p>
<p>So <span class="math inline">\(p(Y) = \int p(Y|X)p(X)dX\)</span> and this was an integral that was too difficult to solve … Wait a minute, isn’t this the marginal likelihood? Yes it is, and the Kalman filter (or a Bayesian filter like the Particle filter) can be used to find this likelihood. This is what those papers I mentioned in the beginning noticed and utilized. They used a Particle filtering approach as Kalman filters restrict you to a Linear Gaussian system (although <span class="citation" data-cites="Bezenac2020">(<a href="#ref-Bezenac2020" role="doc-biblioref">Bézenac et al. 2020</a>)</span> uses a Kalman filter with normalizing flows to transform the Gaussian distribution), but the principle remains the same. Also note that instead of a VAE you would extend it to a DVAE (VAE but for sequential data) as Bayesian filters deal with sequential variables. I just used the VAE because the notation is simpler.</p>
<p>In the previous section mentioned that an optimization routine could be used along with the marginal likelihood as a loss function to optimize the parameters of the model. I also said backprop could be that optimization routine. A filtering algorithm could therefore be used to train a deep learning model based on the criteria defined by the marginal likelihood. This is exactly what we want a generative model to do, maximize the probability the data we generate (let’s call it <span class="math inline">\(\hat{Y}\)</span>) comes from the data distribution <span class="math inline">\(p(Y)\)</span>. With this sequential case and Bayesian filtering formulation this creates a cool way of seeing how we might learn probabilistic dynamic systems using deep learning.</p>
<p>Even if you don’t end up using a filtering algorithm to optimize your DVAE and instead use ELBO; I hope this post has shown you why you are secretly learning the state space dynamics of a system when training these models. A lot of the theory behind generative models went over my head when I was first learning about it. But this really helped me ground my understanding in a theory I was comfortable with and understood relatively well. I hope this post managed to help you in a similar way or at least shown you a different perspective on this topic.</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Bezenac2020" class="csl-entry" role="listitem">
Bézenac, Emmanuel de, Syama Sundar Rangapuram, Konstantinos Benidis, Michael Bohlke-Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, and Tim Januschowski. 2020. <span>“Normalizing Kalman Filters for Multivariate Time Series Analysis.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:2995–3007. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1f47cef5e38c952f94c5d61726027439-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2020/file/1f47cef5e38c952f94c5d61726027439-Paper.pdf</a>.
</div>
<div id="ref-Maddison2017" class="csl-entry" role="listitem">
Maddison, Chris J., Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, and Yee Whye Teh. 2017. <span>“Filtering Variational Objectives,”</span> May. <a href="https://doi.org/10.48550/arxiv.1705.09279">https://doi.org/10.48550/arxiv.1705.09279</a>.
</div>
<div id="ref-Naesseth2017" class="csl-entry" role="listitem">
Naesseth, Christian A., Scott W. Linderman, Rajesh Ranganath, and David M. Blei. 2017. <span>“Variational Sequential Monte Carlo,”</span> May. <a href="https://doi.org/10.48550/arxiv.1705.11140">https://doi.org/10.48550/arxiv.1705.11140</a>.
</div>
<div id="ref-Sarkka2013" class="csl-entry" role="listitem">
Särkkä, Simo. 2013. <span>“Bayesian Filtering and Smoothing.”</span> In <em>Institute of Mathematical Statistics Textbooks</em>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>