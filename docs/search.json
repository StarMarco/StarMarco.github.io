[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Marco Star. I am pursuing a PhD in mechanical engineering, where I mainly focus on using deep learning for machinery prognostics. Basically, I try to construct deep neural networks to estimate the remaining useful life of machines. This blog exists as an exercise for me to write about general topics that interest me (deep learning, generative models, Bayesian filters, etc…). I decided to do this writing in blog form simply for the experience of creating a blog. Although if anyone reads this, I hope the content aids your understanding of these topics. Normally when I try learning something new, I end up with multiple tabs open in my browser, each with different explanations of the topic. Eventually, I end up developing my own way of understanding the material. I would be honoured if this blog ends up as one of your tabs."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Dynamical Variational Autoencoders\n\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2022\n\n\nMarco Star\n\n\n\n\n\n\n  \n\n\n\n\nVariational Autoencoders\n\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2022\n\n\nMarco Star\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DVAE/DVAE.html",
    "href": "posts/DVAE/DVAE.html",
    "title": "Dynamical Variational Autoencoders",
    "section": "",
    "text": "This post is about the Dynamical Variational Autoencoder, which is like the Variational Autoencoder but for sequences of inputs such as time series. It is meant to be the sequel to my post about Variational Autoencoders"
  },
  {
    "objectID": "posts/DVAE/DVAE.html#introduction",
    "href": "posts/DVAE/DVAE.html#introduction",
    "title": "Dynamical Variational Autoencoders",
    "section": "Introduction",
    "text": "Introduction\nPreviously I have gone over Variational Autoencoders and mentioned that they are not suitable in their common form if we want to learn the underlying latent dynamics of a system. The main problem is that the generative model (the prior and decoder) only generates static data and isn’t concerned with the underlying dynamics of the data. For example, if we want to use the VAE to generate a sequence of positions of a moving object \\(X = [x_1,...,x_T]\\) then we would do the following,\n\nSample from the prior, \\(Z = [z_1,...,z_T] \\sim p(Z)\\)\nDecode the samples, \\(p_{\\theta_x}(X|Z)\\) and sample generated input data \\(X\\)\n\nNote, when I refer to the generative model I refer to the joint distribution between the input variables (\\(X\\)) and the latent variables (\\(Z\\)) i.e. \\(p(X,Z)\\). This can be written as \\(p(X,Z) = p(X|Z)p(Z)\\) which is just the decoder and prior of a VAE. Hence, when I refer to the generative model I am referring to these two components of the VAE which can practically be applied using the above steps.\nHere the generative model essentially defines all possible input position sequences \\(X=[x_1,...,x_T]\\). But what if we want to generate any arbitrary length e.g. we have some data and want to forecast possible future positions until they reach some target position. Or what if I want to understand or use the underlying dynamics behind these positions, so I can generate more accurate or plausible sequences. How would I do this?\nLuckily there are a type of VAE called the Dynamical Variational Autoencoder (DVAE) which solves this problem and improves the quality of generated sequential signals when compared to the regular VAE. The same underlying theory from the VAE is still there, in fact the underlying model is pretty much the same we just need to write it in a different mathematical form and we will get the DVAE. I’ll try to go through a lot of the basic theory in this post, but if you want a more in-depth resource on DVAEs, then I highly suggest the following review paper (Girin et al. 2021)."
  },
  {
    "objectID": "posts/DVAE/DVAE.html#theory",
    "href": "posts/DVAE/DVAE.html#theory",
    "title": "Dynamical Variational Autoencoders",
    "section": "Theory",
    "text": "Theory\nSo how do we implement a DVAE? Well lets first look at the components of a regular VAE,\n\nInference model: \\(q_{\\phi}(Z|X)\\)\nGenerative model: \\(p_{\\theta}(X,Z) = p_{\\theta_x}(X|Z) p_{\\theta_z}(Z)\\)\n\nWhere any subscripts on these probability distributions are the learnable parameters of the neural network that represents the distribution e.g. if \\(q_{\\phi}(Z|X)\\) is the neural network with inputs \\(X\\) and outputs \\(Z\\) then \\(\\phi\\) are the learnable parameters that will be optimised during training.\nNote, if you are familiar with VAEs you may wonder why the prior \\(p_{\\theta_z}(Z)\\) has parameters \\(\\theta_z\\) associated with it, as we normally define it as a standard normal distribution (so there is nothing to optimise). This is because in the DVAE formulation it will be a neural network, so it’s a teaser for later (you can ignore it for now).\nNow all we need to do is say that these variables are sequences of some arbitrary length \\(T\\) i.e. \\(X= x_{1:T} = [x_1,...,x_T]\\) and \\(Z = z_{1:T} =[z_1,...,z_T]\\) and substitute this in.\n\nInference model: \\(q_{\\phi}(z_{1:T}|x_{1:T})\\)\nGenerative model: \\(p_{\\theta}(x_{1:T},z_{1:T}) = p_{\\theta_x}(x_{1:T}|z_{1:T}) p_{\\theta_z}(z_{1:T})\\)\n\nOkay, you might be thinking “how has this helped”, well we can now rewrite these probability distributions as products of other probability distributions. Let’s first illustrate this for the generative model.\n\\[\n    \\text{Generative: } p_{\\theta}(x_{1:T}, z_{1:T}) = \\prod_{t=1}^T p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}) p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\n\\]\nNotice now we have this causal model, which is to say variables at time \\(t\\) only depend on past variables \\(1:t-1\\) (the present value is determined by the past values). This means our new decoder and prior models become,\n\\[\n    \\text{Decoder: } p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}) \\\\\n    \\text{Prior: } p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\n\\]\nThe prior generates the next latent variable \\(z_t\\) which can then be used to input all \\(z_{1:t}\\) into the decoder. We can alternate or chain this operation to generate an entire sequence of \\(x_{1:T}\\) for any arbitrary value of \\(T\\) by simply alternating between generating the next \\(z_t\\) value and then generate \\(x_t\\), which helps generate \\(z_{t+1}\\) which generates \\(x_{t+1}\\) and so on.\nBut to train the generative model we need an inference model to encode the inputs to a latent space. When breaking the inference model into a product of distributions we get,\n\\[\n    \\text{Inference: } q_{\\phi}(z_{1:T}|x_{1:T}) = \\prod_{t=1}^T q_{\\phi}(z_t|z_{1:t-1}, x_{1:T})\n\\]\nThis cannot be simplified further; unlike the joint distribution we are stuck with the fact we are given \\(x_{1:T}\\). Hence, this is a noncausal model where future values are needed to calculate present variables at time \\(t\\). Since the inference model is only used when training, \\(x_{1:T}\\) is anyway a given/available to us. This is because it is our training data, so it’s not a big deal that this model is noncausal (we are not cheating by looking into the future). But this does have an important impact on the model which may not be intuitive. As my gut instinct if I had to extend the VAE for sequential inputs would be to use \\(x_{1:t}\\) for the inference model. However, by breaking the probability distribution of the sequence into the product of distributions which represent the current values at time \\(t\\) we can see \\(x_{1:T}\\) is needed for the inference model. But how do we represent entire sequences (which could be any arbitrary length \\(T\\)) so that they can be used as inputs to a neural network?\nThe most common way is to use a Recurrent Neural Network (RNN). Many implementations will input a sequence (e.g. \\(x_{1:t}\\)) and let the hidden variable at that time \\(t\\) be a representation of the entire sequence i.e. \\(h_t := x_{1:t}\\). This can be seen in the diagram below where the RNN cell takes in the previous hidden state (\\(h_{t-1}\\)) and current input value (\\(x_t\\)) and outputs \\(h_t\\). Since each hidden state carries the information of the previous and current input, \\(x_{1:t}\\), the hidden state \\(h_t\\) is used to represent this input sequence. Note theoretically other models could be used to represent the input sequence \\(x_{1:t}\\), but the RNN is chosen due to its simplicity as it represents the entire sequence with just one variable \\(h_t\\). For \\(x_{1:T}\\) we can use a forward-backward RNN, where we essentially run the forward RNN (as shown in the diagram) and then run another backwards (from \\(t=T\\) to \\(t=1\\)) with \\(h_{T:1}\\) and \\(x_{T:1}\\) as the inputs to the RNN (we can concatenate them together in practice, so they are one input). We will use \\(\\overleftarrow{h}_t\\) as the symbol for hidden variable output of a forward-backwards RNN where, \\(\\overleftarrow{h}_t := x_{1:T}\\).\n\nWe now need to construct a network which will represent the inference distribution \\(q_{\\phi}(z_t|z_{1:t-1}, x_{1:T})\\) and the generative model distributions, i.e. the prior \\(p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\\) and the decoder \\(p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t})\\). This can be done by looking at the given values of each distribution and using those variables as the inputs to a neural network. The output of the neural network can be the mean and variance of a Gaussian distribution for simplicity. Hence, could construct the following networks as an attempt to model these distributions,\n\\[\n    \\text{inference network: } f_{\\phi}(z_{1:t-1}, x_{1:T}) = f_{\\phi}(z_{t-1}, \\overleftarrow{h}_t) = [\\mu_{\\hat{z}}, \\sigma_{\\hat{z}}]\n\\] \\[\n    \\text{prior network: } f_{\\theta_z}(z_{1:t-1}, x_{1:t-1}) = f_{\\theta_z}(z_{t-1}, h_{t}) = [\\mu_{z}, \\sigma_{z}]\n\\] \\[\n    \\text{decoder network: } f_{\\theta_x}(x_{1:t-1}, z_{1:t}) = f_{\\theta_x}(h_t, z_t) = [\\mu_{x}, \\sigma_{x}]\n\\]\nWhere each network outputs a mean \\(\\mu\\) and standard deviation \\(\\sigma\\) to define a Gaussian distribution \\(\\mathcal{N}(\\mu, \\sigma)\\), also note how each data sequence (\\(x_{1:t-1}\\)) was replaced by a hidden variable representation using a RNN (\\(x_{1:t-1} := h_{t}\\)). We also assumed for simplicity that the latent dynamics followed a Markov assumption so \\(z_t|z_{1:t-1}\\) became \\(z_t|z_{t-1}\\) i.e. the current variable only depends on the previous time variable. It should be mentioned that from a practical perspective the networks do not usually output the standard deviation but the log-variance (\\(\\text{log}\\sigma^2\\)) as the range is not restricted to only positive values, note you can make the standard deviation an output as long as an activation function is used to ensure the value is positive (e.g. softplus).\nAnother way of representing these equations and designing DVAEs is by using block diagrams. For example, you can see one of the inference model below.\n\nWith this diagram we can see how the data is handled. Each variable inside a circle is a stochastic variable, hence, if an arrow points to a circle we know that the output of the network that outputs that variable would be \\((\\mu, \\sigma)\\) for our example as we are using Gaussian distributions to model random variables. Rectangle blocks are deterministic variables and can therefore be direct outputs of a neural network (all the deterministic variables in the diagram are hidden variable outputs from a RNN). The diagram can make certain concepts clearer e.g. we can see why \\(h_t := x_{1:t-1}\\) as each \\(h_t\\) is generated from inputs \\(x_{t-1}\\) and \\(h_{t-1}\\), since \\(h_{t-1}:=x_{1:t-2}\\), \\(h_t\\) is a union of \\(x_{1:t-2}\\) and \\(x_{t-1}\\) which means it represents \\(x_{1:t-1}\\).\nThe diagram below is for the generative model.\n\nNotice that the generative model ultimately outputs \\(x_t\\) variables using the latent dynamics of \\(z_t\\). We can see this from the diagram through the arrows which point to \\(x_t\\) from \\(h_t\\) and \\(z_t\\) (meaning \\(h_t\\) and \\(z_t\\) are inputs). Since, \\(h_t := x_{1:t-1}\\) we have \\(f_{\\theta_x}(x_{1:t-1},z_t) = x_t\\) which is what we stated before as the decoder model. Hence, these diagrams can be a handy visual representation of the equations we stated previously, and sometimes I find it useful to draw the diagrams as they help me see how the data “flows” through the networks, and then from the diagrams see what my equations and probability distribution representations are.\nFinally, we should look at the loss function used to train the DVAE. Let us first have a look the loss function for a regular VAE i.e. the Evidence Lower Bound (ELBO),\n\\[\n\\text{ELBO} = \\mathbb{E}_{q_{\\phi}(Z|X)}\\left[\\text{log} p_{\\theta_x}(X|Z)\\right] - D_{KL}\\left( q_{\\phi}(Z|X) || p(Z)\\right)\n\\]\nWe can do what we did previously to define the DVAE from the VAE formulation, that is, make the following replacements, \\(X= x_{1:T} = [x_1,...,x_T]\\) and \\(Z = z_{1:T} =[z_1,...,z_T]\\) resulting in,\n\\[\n\\text{ELBO} = \\mathbb{E}_{q_{\\phi}(z_{1:T}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{1:T}|z_{1:T})\\right] - D_{KL}\\left( q_{\\phi}(z_{1:T}|x_{1:T}) || p_{\\theta_z}(z_{1:T})\\right)\n\\]\nFrom the previous analysis we know we can express the distribution of a sequence such as \\(x_{1:T}\\) with a product of distributions defining \\(x_t\\) given the previous \\(x_{1:t-1}\\). Since both loss terms have \\(log\\) acting on the distributions, the product over time period \\(T\\) can become a sum over \\(T\\). The final loss ends up being,\n\\[\n\\text{ELBO} = \\sum_{t=1}^{T} \\mathbb{E}_{q_{\\phi}(z_{1:t}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{t}|x_{1:t-1}, z_{1:t})\\right] - \\sum_{t=1}^T \\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})} \\left[D_{KL}\\left( q_{\\phi}(z_{t}|z_{1:t-1}, x_{1:T}) || p_{\\theta_z}(z_{t}| z_{1:t-1}, x_{1:t-1})\\right)\\right]\n\\]\nNote the expected value is with respect to the inference distribution. Hence, when you see a term like \\(\\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})}\\) is the loss function above, this means the sequence \\(z_{1:t-1}\\) should come from the inference model. For example, the KL-divergence has the term, \\(p_{\\theta_z}(z_{t}| z_{1:t-1}, x_{1:t-1})\\), inside the expection \\(\\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})}\\). This means we use the prior model to generate \\(z_t\\) but the previous latent variables \\(z_{1:t-1}\\) came from the inference model. Another example is the log-likelihood which is \\(\\mathbb{E}_{q_{\\phi}(z_{1:t}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{1:t}|x_{1:t-1}, z_{1:t})\\right]\\). Here we use the decoder \\(p_{\\theta_x}\\) to generate \\(x_t\\) using the previous \\(x_{1:t-1}\\) values and the latent variables again from the inference model as the expectation (\\(\\mathbb{E}\\)) is with respect to the inference distribution \\(q_{\\phi}(z_{1:t}|x_{1:T})\\). Again as a reminder, for a more detailed look at all this theory {% cite Girin2020 %} has an awesome review paper on the topic."
  },
  {
    "objectID": "posts/DVAE/DVAE.html#constructing-a-dvae",
    "href": "posts/DVAE/DVAE.html#constructing-a-dvae",
    "title": "Dynamical Variational Autoencoders",
    "section": "Constructing a DVAE",
    "text": "Constructing a DVAE\nNow lets look at an example of using the DVAE to model a dynamic system. I used the example of a mass-spring system in my post about VAEs, but stated we needed a way of handling sequences to allow the network to find the underlying dynamics of the system. Now that we have the DVAE in our tool kit, we have a generative model well suited to handle sequence inputs. As a reminder the diagram of the mass-spring system is below, where the mass is moving up and down in the z-direction. The idea here is that we pretend we have never seen a mass-spring system before in our lives, and so we set up sensors around the room to measure the distance between the sensor and the mass (these are labelled as \\(x\\) on the diagram). If we knew to set up the sensor in the z-direction we would have a nice 1D representation of the system dynamics, but we don’t know that, so instead we have a bunch of sensor signals which together give us a \\(n\\)-dimentional representation of the system dynamics. Our aim is to use the DVAE to find a lower dimensional representation of the system (it does not necessarily have to be the dynamics in the z-direction).\n\nWe can generate some of these sensor readings by simulating the motion of the system and placing \\(n\\) sensors randomly around a room by specifying their Cartesian coordinates,\n\\[\n    (x, y, z) \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\nHere \\(n=14\\) and we can plot what each sensor looks like as an example of the dynamics of 1 mass-spring system.\n\n\nCode\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sb \nimport torch \nimport torch.nn as nn \n\n%matplotlib inline \nsb.set_theme()\n\n\n\n\nCode\ndef sensor_loc(n):\n    # coordinates of sensors in a cartesian plane (origin is when z=0 before spring-mass system is stretched)\n    x_sen = np.random.normal(size=[n, 1]) \n    y_sen = np.random.normal(size=[n, 1])   \n    z_sen = np.random.normal(size=[n, 1])   \n    return (x_sen, y_sen, z_sen)\n\ndef get_sensor_values(sen_loc, t_min, t_max, res=100, m=1., k=1., z0=1.):\n    t = np.linspace(t_min, t_max, res)\n    z = z0*np.cos(np.sqrt(k/m) * t)     # spring mass motion along the z-axis \n\n    x_sen, y_sen, z_sen = sen_loc\n\n    z_diff = z - z_sen  # only difference in the z-axis\n    xs = np.sqrt(x_sen**2 + y_sen**2 + z_diff**2)\n\n    return t, xs.T     # shape (time, n)\n\nsensors = 14\nsen_loc = sensor_loc(sensors)\nt, X = get_sensor_values(sen_loc, 0, 20, res=200)\n\nplt.figure(figsize=(18,9))\nplt.plot(t, X)\nplt.show()\n\n\n\n\n\nTo train a DVAE we will need a whole dataset of mass-spring system dynamics. Hence, I will simulate 100 different mass-spring systems with slightly different mass, spring constant and initial values for training and another 100 for testing the model. From now on I will refer to each different mass-spring system as a “unit” (as its easier to type). Hence, we have 100 different training units and 100 testing units. For model training, I have also used a sliding window so inputs into the model are “packets” of the signal with a time window size \\(T=40\\). So inputs are tensors of size (batch size, T, sensors) while the testing inputs are still the entire time sequence. The training data is split into these time windowed packets, simply so we have more data that can be computed in parallel which allows for quicker training on a GPU. Each sensor signal is also normalised to have a mean of zero and a standard deviation of one. This helps with training the network as the signals don’t have varying magnitudes, so the network can stick to a known range.\n\n\nCode\ndef get_data(units, sensor_loc, t_min=0, t_max=20, res=200):\n    data = [] \n\n    for _ in range(units):\n        m = np.random.rand()\n        k = np.random.rand()\n        z0 = np.random.rand()\n\n        t, X = get_sensor_values(sen_loc=sensor_loc, t_min=t_min, t_max=t_max, res=res, m=m, k=k, z0=z0)   # generate data for different spring-mass systems \n\n        # Normalize data to improve training \n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n        data.append(torch.tensor(X))\n\n    data = torch.stack(data, dim=0)     # shape (units, time, No. of sensors) \n    return data \n\ntrain_units = 100\ntest_units = 100 \ntrain_data = get_data(train_units, sensor_loc=sen_loc)\ntest_data = get_data(test_units, sensor_loc=sen_loc)\n\n\n\n\nCode\nplt.figure(figsize=(18,9))\nplt.plot(train_data[0])\nplt.show()\n\n\n\n\n\n\n\nCode\ndef prep_data(data, T, bs):\n    data = data.unfold(1, T, 1).permute(0,1,3,2)                    # (n, units, T, dim)\n    data = torch.reshape(data, [-1,data.shape[-2],data.shape[-1]])  # (n*units, T, dim)\n\n    train_DataSet = torch.utils.data.TensorDataset(data, data)\n    train_loader = torch.utils.data.DataLoader(train_DataSet, shuffle=True, batch_size=bs)\n    return train_loader \n\nT = 40\nbs = 150 \ntrain_loader = prep_data(train_data, 40, bs)\n\n\nHere I simply have some code that defines some utilities like a simple Feedforward or Multilayer Perceptron (MLP) neural network as well as some loss functions needed in the DVAE i.e. the KL divergence between two Gaussian distributions and the log-likelihood of a Gaussian distribution.\n\n\nCode\n# -------------------\n# Neural Network Utils\n# -------------------\nclass MLP(nn.Module):\n  def __init__(self, input_dim, hidden_dim, output_dim):\n    super(MLP, self).__init__()\n    self.linear1 = nn.Linear(input_dim, hidden_dim)\n    self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n    self.linear3 = nn.Linear(hidden_dim, output_dim, bias=None)\n\n    self.nonlinearity = torch.relu\n\n  def forward(self, x):\n    h = self.nonlinearity( self.linear1(x) )\n    h = h + self.nonlinearity( self.linear2(h) )\n    return self.linear3(h)\n\n# --------------\n# Loss \n# --------------\ndef loss_KLD(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    KL-divergance between 2 Gaussian distributions, given the mean and log-variance of \n    Gaussian 1 and Gaussian 2. \n    \"\"\"\n    loss = -0.5 * torch.sum(logvar1 - logvar2\n    - torch.div(logvar1.exp() + (mean1 - mean2) ** 2, logvar2.exp() + 1e-10))\n    return loss \n\ndef loglike(x, mean, lvar):\n    var = torch.exp(lvar) \n    return -0.5 * torch.log(2*(np.pi)*var) - (x - mean) ** 2 / (2*var)\n\n\nThe following is code for the actual DVAE itself.\n\n\nCode\nclass DVAE(nn.Module):\n    def __init__(self, xdim, zdim, hdim):\n        super().__init__()\n        self.xdim = xdim \n        self.zdim = zdim \n        self.hdim = hdim \n\n        # deterministic (RNNs)\n        self.x_fwd = nn.GRU(xdim, hdim, batch_first=True)\n        self.x_bck = nn.GRU(xdim+hdim, hdim, batch_first=True)\n\n        # latent \n        self.z_gen = MLP(hdim+zdim, hdim*2, hdim) # prior\n        self.z_inf = nn.GRU(hdim, zdim*2, batch_first=True) # inference model \n\n        self.prior_mean = MLP(hdim, hdim, zdim)\n        self.prior_lvar = MLP(hdim, hdim, zdim)\n        \n        self.inf_mean = MLP(hdim, hdim, zdim)\n        self.inf_lvar = MLP(hdim, hdim, zdim)\n\n        # generative \n        self.x_dec = MLP(zdim+hdim, hdim, xdim*2)   # decoder\n\n    # --- Helper functions --- \n    def sample(self, mean, lvar):\n        std = torch.exp(0.5 * lvar)\n        eps = torch.randn(mean.size()).to(mean.device)\n        return mean + eps * std \n\n    def get_stats(self, stats):\n        dim = stats.shape[-1] // 2 \n        mean = stats[...,:dim]\n        lvar = stats[...,dim:]\n        return mean, lvar \n\n    ## --- Used for Training --- \n    def decode(self, hs, zs):\n        hzs = torch.cat((hs, zs), dim=-1)\n        x_stats = self.x_dec(hzs)\n\n        x_mean, x_lvar = self.get_stats(x_stats)\n        return x_mean, x_lvar \n\n    def represent_x(self, xs):\n        bs, _, xdim = xs.shape\n\n        x0 = torch.zeros(bs, 1, xdim).to(xs.device) # dummy init. var.\n        x_tm1 = torch.cat((x0, xs[:,:-1,:]), 1) # tm1 = current time minus 1 = previous time variable \n        hs, _ = self.x_fwd(x_tm1)\n        return hs\n\n    def inference(self, xs):\n        # --- Encode inputs to represent sequences x_{1:T} ---\n        self.hs = self.represent_x(xs)\n\n        h_x = torch.cat((self.hs, xs), dim=-1)\n        g_revs, _ = self.x_bck(torch.flip(h_x, [1]))    # backward RNN \n        gs = torch.flip(g_revs, [1]) \n\n        # --- Latent ---\n        z_stats, _ = self.z_inf(gs)\n        z_means, z_lvars = self.get_stats(z_stats)\n        zs = self.sample(z_means, z_lvars)\n        \n        return zs, z_means, z_lvars\n\n    def generate_z(self, hs, z_tm1):\n        hzs = torch.cat((hs, z_tm1), dim=-1)\n        hzs = self.z_gen(hzs)\n\n        z_means = self.prior_mean(hzs)\n        z_lvars = self.prior_lvar(hzs)\n        return z_means, z_lvars\n\n    def generate_x(self, zs, use_pred=1):\n        bs, seq, _ = zs.shape\n\n        x_tm1 = torch.zeros(bs, 1, self.xdim).to(zs.device)    # dummy init. var. \n        h_tm1 = torch.zeros(1, bs, self.hdim).to(zs.device)\n\n        x_means = []\n        x_lvars = [] \n        hs = []\n\n        for t in range(seq):\n            h, h_tm1 = self.x_fwd(x_tm1, h_tm1)   # h.size = (bs, 1, hdim), h_tm1.size = (1, bs, hdim)\n\n            # during training randomly replace x_{1:t} estimates with ground truth\n            random_num = np.random.uniform()\n            if random_num &lt;= use_pred:\n                h = h # use prediction\n            else:\n                h = self.hs[:,t,:].unsqueeze(1) # use ground truth\n\n            # estimate x \n            x_mean, x_lvar = self.decode(h, zs[:,t,:].unsqueeze(1))   # size (bs, 1, xdim)\n            x_tm1 = self.sample(x_mean, x_lvar)\n\n            x_means.append(x_mean)\n            x_lvars.append(x_lvar)\n            hs.append(h)\n        \n        x_means = torch.cat(x_means, dim=1)\n        x_lvars = torch.cat(x_lvars, dim=1)\n        hs = torch.cat(hs, dim=1)\n\n        return x_means, x_lvars, hs \n\n    def forward(self, xs, use_pred=1):\n        bs = xs.shape[0]\n\n        # generate latent sequence from training data using the inference model \n        zs_inf, zs_inf_mean, zs_inf_lvar = self.inference(xs)\n\n        # use latent sequence from inference model to estimate x-values \n        x_mean, x_lvar, hs = self.generate_x(zs_inf, use_pred=use_pred)\n\n        # generate prior z-values \n        z0 = torch.zeros(bs, 1, self.zdim).to(xs.device)\n        z_tm1 = torch.cat((z0, zs_inf[:,:-1,:]), dim=1)\n        zs_gen_mean, zs_gen_lvar = self.generate_z(hs, z_tm1)\n\n        # KL-div between inference and prior z-values \n        kl = loss_KLD(zs_gen_mean, zs_gen_lvar, zs_inf_mean, zs_inf_lvar) / bs \n\n        return kl, x_mean, x_lvar \n\n    def get_loss(self, xs, use_pred=1):\n        kl, x_mean, x_lvar = self.forward(xs, use_pred=use_pred)\n        nll = -loglike(xs, x_mean, x_lvar).mean()\n        return kl, nll\n    \n    ## --- Used for Testing ---\n    def generate(self, xs, return_hidden=False):\n        '''\n        Used to test reconstructions of xs given xs itself \n        '''\n        bs, seq, xdim = xs.shape\n\n        # --- Encode inputs to represent sequences x_{1:T} ---\n        x0 = torch.zeros([bs, 1, xdim]).to(xs.device)\n        x_tm1 = torch.cat((x0, xs[:,:-1,:]), dim=-2)  # x_{t-1} outputs h_t\n        hs, _ = self.x_fwd(x_tm1)     # forward RNN outputs h_{1:T} given x_{0:T-1}\n\n        # --- Storage --- \n        z_means = [] \n        z_lvars = [] \n        zs = [] \n        \n        # --- Latent ---\n        z_tm1 = torch.zeros(bs, self.zdim).to(xs.device)\n        for t in range(seq):\n            hzs = torch.cat((hs[:,t,:], z_tm1), dim=-1) # (bs, hdim+zdim)\n            hzs = self.z_gen(hzs)\n\n            z_mean = self.prior_mean(hzs)\n            z_lvar = self.prior_lvar(hzs)\n            z = self.sample(z_mean, z_lvar)\n\n            z_means.append(z_mean)\n            z_lvars.append(z_lvar)\n            zs.append(z)\n\n        z_means = torch.stack(z_means, dim=1)\n        z_lvars = torch.stack(z_lvars, dim=1)\n        zs = torch.stack(zs, dim=1)\n\n        # --- Generate/Decode --- \n        x_mean, x_lvar = self.decode(hs, zs)\n\n        if return_hidden:\n            return x_mean, x_lvar, z_means, z_lvars, hs\n\n        return x_mean, x_lvar, z_means, z_lvars\n\n    def generate_rec(self, x0, h0, z0_mean, z0_lvar):\n        '''\n        generates given time = t-1 values generate values at time = t \n        '''\n        # generate values reccurently i.e. given values @ t-1 generates values @ t \n        h1, h = self.x_fwd(x0, h0)\n        z0 = self.sample(z0_mean, z0_lvar)\n        hz = torch.cat((h1, z0), dim=-1)\n\n        hz = self.z_gen(hz)\n        z_mean = self.prior_mean(hz)\n        z_lvar = self.prior_lvar(hz)\n        z1 = self.sample(z_mean, z_lvar)\n\n        x_mean, x_lvar = self.decode(h1, z1)\n\n        return x_mean, x_lvar, z_mean, z_lvar, h\n\n    def overshoot(self, xs, split=None):\n        \"\"\"\n        Performs sequential overshooting. \n        i.e. randomly chooses a cut-off point in the data sequence xs and splits \n        it into a known and unknown sequence. Then uses the known sequence to extrapolate\n        the rest of the unknown sequence and compares with the true result. \n        \n        e.g. \n        xs = [x1, x2, x3, x4, x5, x6]\n        known_xs = [x1, x2]\n        unknown_xs = [x3, x4, x5, x6]\n\n        generate_known_xs = [x1_gen, x2_gen]\n\n        using x2_gen as the initial condition \n        extrapolated_xs = [x3_gen, x4_gen, x5_gen, x6_gen]\n\n        xs_gen = [x1_gen, x2_gen, x3_gen, x4_gen, x5_gen, x6_gen]\n\n        overshoot_loss = loglike(xs, xs_gen_mean, xs_gen_lvar)\n        \"\"\"\n        seq = xs.shape[1]\n        if split == None:\n            percent = np.random.uniform(0.1)\n            known_seq = int(seq * percent)\n        else:\n            known_seq = int(seq * split)\n        T = seq - known_seq    # extrapolation length \n\n        # storing \n        xs_means = []\n        xs_lvars = [] \n        zs_means = [] \n        zs_lvars = [] \n        def store(x_mean, x_lvar, z_mean, z_lvar):\n            xs_means.append(x_mean)\n            xs_lvars.append(x_lvar)\n            zs_means.append(z_mean)\n            zs_lvars.append(z_lvar)\n            return xs_means, xs_lvars, zs_means, zs_lvars\n        \n        # generating and extrapolating values \n        obs = xs[:,:known_seq,:]\n        x_mean, x_lvar, z_mean, z_lvar, hs = self.generate(obs, return_hidden=True)\n        xs_means, xs_lvars, zs_means, zs_lvars = store(x_mean, x_lvar, z_mean, z_lvar)\n\n        x = self.sample(x_mean, x_lvar)\n        x = x[:,-1:,:]\n        h = hs[:,-1,:].unsqueeze(0) \n        z_mean = z_mean[:,-1:,:]\n        z_lvar = z_lvar[:,-1:,:]\n        for t in range(T):\n            x_mean, x_lvar, z_mean, z_lvar, h = self.generate_rec(x, h, z_mean, z_lvar)\n            xs_means, xs_lvars, zs_means, zs_lvars = store(x_mean, x_lvar, z_mean, z_lvar)\n\n            x = self.sample(x_mean, x_lvar)\n\n        xs_means = torch.cat(xs_means, dim=1)\n        xs_lvars = torch.cat(xs_lvars, dim=1)\n        zs_means = torch.cat(zs_means, dim=1)\n        zs_lvars = torch.cat(zs_lvars, dim=1)\n\n        return xs_means, xs_lvars, zs_means, zs_lvars\n\n\n\nNow that we have defined all the components we can train the model.\n\n\nCode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nxdim = sensors \nhdim = 50 \nzdim = 1\n\nmodel = DVAE(xdim, zdim, hdim).to(device)\n\nlr = 1e-3 \noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n\n\n\nCode\ndef train_model(model, optimizer, train_loader, epochs, use_pred=1):\n    for epoch in range(1, epochs+1):\n        for xs, _ in train_loader:\n            xs = xs.float().to(device)\n\n            kl, nll = model.get_loss(xs, use_pred)\n\n            elbo = nll + kl\n\n            elbo.backward()\n            optimizer.step()\n            optimizer.zero_grad() \n\n        if epoch == 1 or (epoch % 10) == 0: \n            print(\"Epoch: {}/{}, elbo: {:.4f}, nll: {:.4f}, kl: {:.4f}\"\n            .format(epoch, epochs, elbo, nll, kl))\n\n\n\n\nCode\ntrain_model(model, optimizer, train_loader, epochs=100, use_pred=0.5)\n\n\nEpoch: 1/100, elbo: 20.8290, nll: 0.8044, kl: 20.0246\nEpoch: 10/100, elbo: 20.5449, nll: 0.5424, kl: 20.0025\nEpoch: 20/100, elbo: 19.2262, nll: -0.7744, kl: 20.0007\nEpoch: 30/100, elbo: 20.4248, nll: 0.4242, kl: 20.0005\nEpoch: 40/100, elbo: 19.9360, nll: -0.0643, kl: 20.0003\nEpoch: 50/100, elbo: 19.3641, nll: -0.6360, kl: 20.0002\nEpoch: 60/100, elbo: 18.6368, nll: -1.3633, kl: 20.0001\nEpoch: 70/100, elbo: 18.8004, nll: -1.2230, kl: 20.0234\nEpoch: 80/100, elbo: 19.8288, nll: -0.1713, kl: 20.0001\nEpoch: 90/100, elbo: 18.4936, nll: -1.5064, kl: 20.0000\nEpoch: 100/100, elbo: 18.1595, nll: -1.8405, kl: 20.0000\n\n\nHere we will test the DVAE by first seeing if the generative model can simply reconstruct the sensor signals given. Essentially we will give it the true \\(x_{t-1}\\) value and it will generate \\(z_t\\), which will be decoded to \\(x_t\\). This will be done for all \\(t \\in [0,T]\\) where \\(T\\) is the entire sequence (instead of just a time window like in training). Since \\(x_0\\) is a dummy variable I will ignore it when plotting the sensors.\n\n\nCode\ndef test_model(model, test_data):\n    results = {\n        \"x_true\": [], \n        \"x_mean\": [],\n        \"x_stds\": [],\n        \"z_mean\": [],\n        \"z_stds\": [], \n        \"rmse\": [],\n    }\n\n    for xs in test_data:\n        xs = xs.unsqueeze(0).float().to(device)\n        \n        x_mean, x_lvar, z_mean, z_lvar = model.generate(xs)\n        \n        RMSE = ((x_mean - xs) ** 2).mean()\n        x_stds = torch.exp(0.5 * x_lvar)\n        z_stds = torch.exp(0.5 * z_lvar)\n        \n        results[\"x_true\"].append(xs.detach().cpu().numpy())\n        results[\"x_mean\"].append(x_mean.detach().cpu().numpy())\n        results[\"x_stds\"].append(x_stds.detach().cpu().numpy())\n        results[\"z_mean\"].append(z_mean.detach().cpu().numpy())\n        results[\"z_stds\"].append(z_stds.detach().cpu().numpy())\n        results[\"rmse\"].append(RMSE.detach().cpu().numpy())\n\n    return results \n\n\n\n\nCode\nresults = test_model(model, test_data)\nprint(\"AVG RMSE: \", sum(results[\"rmse\"]) / len(results[\"rmse\"]))\n\n\nAVG RMSE:  0.039877471905201675\n\n\nI only plot one sensor below otherwise I find it gets to cluttered and difficult to see how well it performed.\n\n\nCode\nunit = 11 \n# since x0 = tensor of zeros the first value can be quite off, but once real data comes in we can see it tracks the signal nicely \nx_true = results[\"x_true\"][unit-1][0,1:,:]\nx_mean = results[\"x_mean\"][unit-1][0,1:,:]\nz_mean = results[\"z_mean\"][unit-1][0,1:,:]\nz_stds = results[\"z_stds\"][unit-1][0,1:,:]\n\ns = 10   # sensor number \nplt.figure(figsize=(18,9))\nplt.plot(x_true[...,s-1])\nplt.plot(x_mean[...,s-1])\nplt.show()\n\n\n\n\n\nWe can also plot the latent variable sequences we generated when we reconstructed all these sensors.\n\n\nCode\nplt.figure(figsize=(18,9))\nplt.plot(z_mean)\nplt.show()\n\n\n\n\n\nPlotting all the latent variable sequences we get.\n\n\nCode\n#collapse-hide\nplt.figure(figsize=(18,9))\nfor i in range(100):\n    plt.plot(results[\"z_mean\"][i][0,1:,:])\nplt.show()\n\n\n\n\n\nYou could imagine we could interpolate a new latent sequence in this latent space using our own initial condition and the prior latent dynamics. If we did this and decoded it back to the input space we should get new reconstructed sensor values of some fictitious mass-spring system that doesn’t exist (I’m aware none of these systems existed as I simulated them, but if this was all done through real life experiments I could simulate a new realistic looking virtual experiment). This could be applied in interesting applications such as generating audio, where we use real audio signals to train the model and then generate new audio using the trained model. We can also use a conditional DVAE where we generate new inputs conditioned on other variables. This can be used in machinery prognostics where you might want to find the Remaining Useful Life (RUL) of a machine or component. The RUL could be the generated inputs and these can be conditioned on sensor values. This would change the DVAE model to,\n\\[\n    \\text{Encoder/Inference: } q_{\\phi}(z_t|z_{1:t-1}, x_{1:T}, u_{1:T})\n\\] \\[\n    \\text{Decoder: } p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}, u_{1:t})\n\\] \\[\n    \\text{Prior: } p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1}, u_{1:t})\n\\]\nwhere in this example \\(x_t = (RUL)_t\\) and $u_t = $ sensor signals at time \\(t\\).\nThe code below shows how one would generate new signals from “virtual” mass-spring systems that were not seen during training.\n\n\nCode\ndef generate_sensors(model, xs, split, device):\n    results = {\n        \"x_mean\": [], \n        \"x_stds\": [],\n        \"z_mean\": [],\n        \"z_stds\": []\n    }    \n    xs = xs.unsqueeze(0).float().to(device)\n    def add_to_results(results, x_mean, x_lvar, z_mean, z_lvar):\n        x_stds = torch.exp(0.5 * x_lvar)\n        z_stds = torch.exp(0.5 * z_lvar)\n\n        results[\"x_mean\"].append(x_mean.detach().cpu())\n        results[\"x_stds\"].append(x_stds.detach().cpu())\n        results[\"z_mean\"].append(z_mean.detach().cpu())\n        results[\"z_stds\"].append(z_stds.detach().cpu())\n        return results \n\n    x_mean, x_lvar, z_mean, z_lvar = model.overshoot(xs, split)\n    results = add_to_results(results, x_mean, x_lvar, z_mean, z_lvar)\n\n    results[\"x_mean\"] = torch.cat(results[\"x_mean\"], dim=1).numpy()\n    results[\"x_stds\"] = torch.cat(results[\"x_stds\"], dim=1).numpy()\n    results[\"z_mean\"] = torch.cat(results[\"z_mean\"], dim=1).numpy()\n    results[\"z_stds\"] = torch.cat(results[\"z_stds\"], dim=1).numpy()\n\n    return results \n\nunit = 10\nsplit = 0.7\nseq = test_data[unit-1].shape[0]\n\ngen_results = generate_sensors(model, test_data[unit-1], split, device=device)\n\n\nHere we give our model 70% of the sensors and ask it to extrapolate the 30% we didn’t give it. We can plot the results below.\n\n\nCode\nz_mean = gen_results[\"z_mean\"][0,1:,0]\nt = torch.linspace(0, z_mean.shape[0], z_mean.shape[0])\n\nplt.figure(figsize=(18,9))\nplt.plot(t, z_mean)\nplt.show()\n\n\n\n\n\n\n\nCode\ns = 7 # sensor number \nx_mean = gen_results[\"x_mean\"][0,1:,s]\nx_stds = gen_results[\"x_stds\"][0,1:,s]\n\nplt.figure(figsize=(18,9))\nplt.plot(t, x_mean, label=\"estimate\")\nplt.plot(t, test_data[unit-1][1:,s], label=\"true\")\nplt.fill_between(t, x_mean + 3*x_stds, x_mean - 3*x_stds, alpha=0.3)\nplt.legend()\nplt.show()\n\n\n\n\n\nThis doesn’t look too bad, but each time we are sampling a different \\(z_t\\) value at each time and decoding it. Hence, each time we call the function to extrapolate we generate a different trajectory \\(z_{1:T}\\) and decode to \\(x_{1:T}\\). So lets run the same code again and get an idea of how much the extrapolated results can vary.\n\n\nCode\nunit = 10\nsplit = 0.7\nseq = test_data[unit-1].shape[0]\n\ngen_results = generate_sensors(model, test_data[unit-1], split, device=device)\n\n\n\n\nCode\ns = 1\n\nplt.figure(figsize=(18,9))\nplt.plot(gen_results[\"x_mean\"][0,1:,s], label=\"estimate\")\nplt.plot(test_data[unit-1][1:,s], label=\"true\")\nplt.legend()\nplt.show()\n\n\n\n\n\nAs you can see the trajectory still maintains the oscillating dynamics but can have a lot of variation when extrapolating. But we can generate signals which look somewhat like the original signal in the sense certain characteristics (such as the oscillations) are captured by the underlying prior latent dynamics. The latent dynamics could be smoother if we played around with the prior model. Here we just used a standard feedforward network which took \\(z_{t-1}\\) to \\(z_{t}\\). To improve the results perhaps a better prior model could be used with more structured dynamics. But hopefully the theory and the simple example (relatively speaking) helps you if you want to use these for an application. So we at least finish this post with an idea of how to improve the model and a cool way of generating dynamic data.\nI think this is a good place to stop. Ultimately the aim of these posts is to build up to the point where we are using DVAEs to estimate the RUL of machinery based on input sensor signals. So the theory and basic examples shown here is to get everyone comfortable with the basic ideas behind DVAEs before we move on to more practical applications. Hopefully this post has aided your understanding of DVAEs (and/or introduced you to a cool new technique), I know the mass-spring system analogy really helped me think about how DVAEs are VAEs but extended to deal with dynamic systems or sequential data."
  },
  {
    "objectID": "posts/VAE/VAE.html",
    "href": "posts/VAE/VAE.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Variational Autoencoders (VAEs) have been covered in many different texts and blog posts. The goal here is not so much to repeat the information covered by so many others but instead to look at the VAE with the goal of introducing Dynamical Variational Autoencoders (DVAEs) later. Ultimately the aim is to cover how learn latent dynamics that describe the behaviour of complex systems. *Note, on the off chance someone is actually reading this (thank you for your interest) but this is my first blog post and so this content may change as I experiment with features and use this notebook as a prototype for future posts. So consider this a draft post for now"
  },
  {
    "objectID": "posts/VAE/VAE.html#a-simple-example",
    "href": "posts/VAE/VAE.html#a-simple-example",
    "title": "Variational Autoencoders",
    "section": "A Simple Example",
    "text": "A Simple Example\nA benefit of VAEs (Kingma and Welling 2014) is that they can take input values (\\(X\\)) and map them to a lower dimensional latent variable (\\(Z\\)). We will look at an example to show this more clearly; it will also be used in a later post to show how to model latent dynamics of systems with a lot of input variables 1. The example shown below is a mass that is suspended from the ceiling. Imagine I pull it in the \\(z\\)-direction and let go. You probably imagine the mass will bob up and down but imagine we don’t know that. Instead, we are alien scientists trying to uncover the hidden dynamics of this complex system we have never seen before. So we set up \\(n\\) cameras or motion sensors all around the room (where \\(n\\) is just some arbitrary number). Each sensor provides a different axis or perspective describing the motion of the spring-mass system.\n\nLet us formally define inputs \\(X = [x_1, x_2, x_3, ..., x_n]\\) which are our sensor readings. In fact, to get an idea of the complexity of the dynamics let us plot what some of these sensor readings may look like. We will say the spring mass system is modelled as,\n\\[\n    z(t) = z_0 \\text{cos}\\left(\\sqrt{\\frac{k}{m}} t\\right)\n\\] where we will simply assume, \\(z_0=k=m=1\\) (initial position, spring constant and mass equal 1). For plotting what the sensor reading may look like, we will assume that the sensors are randomly located around the spring-mass system. The sensors will return the distance between the mass and the sensor as it is in motion. Hence, we will randomly define the Cartesian coordinates of the sensors in the room and simply take the distance between the moving spring-mass system and the sensor locations as the sensor outputs.\n\n\nCode\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sb \n\n%matplotlib inline \nsb.set_theme()\n\n\n\n\nCode\ndef get_sensor_values(n, t_min, t_max, res=100, m=1., k=1., z0=1.):\n    t = np.linspace(t_min, t_max, res)\n    z = z0*np.cos(np.sqrt(k/m) * t)     # spring mass motion along the z-axis \n\n    # coordinates of sensors in a cartesian plane (origin is when z=0 before spring-mass system is stretched)\n    x_sen = np.random.normal(size=[n, 1]) \n    y_sen = np.random.normal(size=[n, 1])   \n    z_sen = np.random.normal(size=[n, 1])   \n\n    z_diff = z - z_sen  # only difference in the z-axis\n    xs = np.sqrt(x_sen**2 + y_sen**2 + z_diff**2)\n\n    return t, xs.T     # shape (time, n)\n\nt, X = get_sensor_values(10, 0, 100, res=1000)\n\nplt.figure(figsize=(18,9))\nplt.plot(t, X)\nplt.show()\n\n\n\n\n\nWe get some relatively weird looking data at least compared to the simple oscillating motion we would get if we only looked at the z-axis motion. The aim of the VAE is to find that simple oscillating motion from the data collected by all these sensors. With this simple system that we can compress all the input data \\(X\\) into a more simple latent space \\(Z\\). In this latent space the motion isn’t as complex and only requires 1 axis to fully describe the motion. The idea of the VAE is to represent high dimensional data in a representative latent space \\(Z\\) which tries to efficiently encode the input data into a lower dimensional space (in this case the latent space \\(Z\\) would fully be able to describe the motion of the system). But how does the VAE do this?"
  },
  {
    "objectID": "posts/VAE/VAE.html#the-components-of-the-vae",
    "href": "posts/VAE/VAE.html#the-components-of-the-vae",
    "title": "Variational Autoencoders",
    "section": "The Components of the VAE",
    "text": "The Components of the VAE\nThe VAE is a generative model. This means instead of simply learning an input \\(\\rightarrow\\) output relationship e.g. \\(f(X) = Z\\) or using probability distributions \\(p(Z|X)\\), we learn the joint probability between the inputs and latent variables \\(p(X,Z)\\). This allows us to transfer from one space to another as \\(p(X,Z) = p(Z|X)p(X)=p(X|Z)p(Z)\\) it also lets us generate inputs \\(X\\) simply by sampling the latent space \\(Z\\) and using \\(p(X|Z)\\) to generate new inputs \\(X\\). But how does the VAE do this?\nFirstly, if the goal is to model the joint distribution \\(p(X,Z)\\) which can be described using \\(p(X,Z) = p(Z|X)p(X)=p(X|Z)p(Z)\\), then since this is deep learning, let us model what we can using neural networks. We then have,\n\n\\(p(Z|X) = q_{\\phi}(Z|X)\\) is the encoder network as it encodes inputs to the latent space. \\(\\phi\\) are the parameters of the network and in most cases it models a Gaussian/Normal distribution by making the network output a mean and log-variance i.e. \\(q_{\\phi}(Z|X) = [\\mu_z, \\text{log}\\sigma_z^2]\\)\n\\(p(X|Z) = p_{\\theta_x}(X|Z)\\) is the decoder as it takes latent variables and transforms them back into the input space. The network parameters are \\(\\theta_x\\) and it models a Gaussian distribution like the encoder\n\\(p(Z) = p_{\\theta_z}(Z)\\) for VAEs this isn’t usually a network but instead we define it as a standard normal distribution \\(\\mathcal{N}(0,I)\\). The idea behind this is that we constain the latent space to this domain so after training we don’t need the encoder to tell us where the latent variables exist. Instead, we just sample from the defined distribution \\(Z \\sim \\mathcal{N}(0,I)\\) and decode to the input space to generate new input data.\n\\(p(X)\\) this is the distribution of the input data, we don’t know what this is and that is the motivation for trying our very best to get rid of it in any formula we see (out of sight out of mind). We will use all sorts of mathematical tricks to derive a loss function which does not include this term.\n\nTo train the VAE first notice that the encoding distribution can be expressed in the following form.\n\\[\n    p(Z|X) = \\frac{p(X|Z)p(Z)}{p(X)}\n\\]\nSince we don’t know what \\(p(X)\\) is we use variational techniques to estimate the unknown distribution \\(p(Z|X)\\) using a simpler distribution (usually a Gaussian distribution). Hence, we have,\n\\[\n    p(Z|X) \\approx q_{\\phi}(Z|X) = [\\mu_z, \\text{log}\\sigma_z^2] \\rightarrow \\mathcal{N}(\\mu_z, \\sigma_z^2)\n\\]\nTo derive the loss (known as the Evidence Lower Bound or ELBO) we try to minimise the difference between the simple distribution \\(q_\\phi(Z|X)\\) and the complex unknown \\(p(Z|X)\\). i.e. \\[\n    D_{KL}(q_\\phi(Z|X)||p(Z|X))\n\\] Where \\(D_{KL}\\) is the KL-divergance.\nThere are many resources which derive the ELBO, so I won’t go over the mathematical details. I will however show the final form of the ELBO and discuss the logic behind each term so that it may become clear how we can later leverage this to learn the latent dynamics of the system (when looking at DVAEs).\n\\[\n    \\text{ELBO} = \\mathbb{E}_{q_{\\phi}(Z|X)}\\left[\\text{log} p_{\\theta_x}(X|Z)\\right] - D_{KL}\\left( q_{\\phi}(Z|X) || p(Z)\\right)\n\\]\nTo optimise this we maximise the ELBO (or we minimise the -ELBO) loss by tuning the parameters network parameters \\(\\phi\\) and \\(\\theta_x\\). Let’s look at the first term, \\[\n    \\mathbb{E}_{q_{\\phi}(Z|X)}\\left[\\text{log} p_{\\theta_x}(X|Z)\\right]\n\\]\nThis is the log-likelihood which determines the likelihood of the decoded latent variable \\(Z\\) to match the target input \\(X\\). This is what we might think of when we think of a traditional or basic loss function; is the output of our network close to the target output we expect. The more interesting term here is the second one,\n\\[\n    D_{KL}\\left( q_{\\phi}(Z|X) || p(Z)\\right)\n\\]\nThis term optimises \\(q_{\\phi}(Z|X)\\) to encode the latent variables \\(Z\\) into a defined space \\(p(Z)\\) (for VAEs this is commonly described using the standard normal distribution). This means an optimised VAE has a latent space constrained to the prior distribution \\(p(Z)\\). Hence, we could theoretically design a simple prior (this is what we do by picking a standard normal distribution) and simplify the latent space, so it is easy to interpret."
  },
  {
    "objectID": "posts/VAE/VAE.html#problems",
    "href": "posts/VAE/VAE.html#problems",
    "title": "Variational Autoencoders",
    "section": "Problems",
    "text": "Problems\nFor learning the underlying dynamics of sequential input data the VAE is not the ideal model. One problem we have is that \\(Z\\) and \\(X\\) are fixed lengths. If we want to learn the dynamics we don’t it is not very useful if we can only deal with sequences of a certain arbitrary length that we choose to train with. e.g. if we trained with input time-series sequences of length \\(T\\), \\(X = [x_1,...,x_T]\\), then we can only reconstruct other \\(X\\) variables of the same length. Hence, if \\(X\\) denotes a list of positions then we are only outputting possible positions the spring-mass system could be in and not understanding the physics or dynamics behind how those positions came about. So VAEs can handle “static” cases where the data is a fixed length or size (e.g. generating images). However, if we want to learn the dynamics of the system we need to be able to handle the dynamic case. In this dynamic case the data could have different sequence lengths, but even if it isn’t, we don’t want to just learn all the possible positions of the spring-mass system, we also want to understand the underlying dynamics that determine how those positions change over time. Another problem is the prior of the VAE is generally a Gaussian distribution with zero mean and unit variance (standard Normal distribution). This is not ideal if we want to learn a sequence as the latent space has no sequential structure. If we use a standard Normal distribution then we are trying to construct a latent sequence which consists of sequences of randomly generated variables that have no relation to each other with respect to time i.e. \n\\[\n    z_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\nHence, the latent space consists of sequences \\(\\{z_t\\}_{t=0}^T\\) where each \\(z_t\\) was randomly generated and is not related to any other \\(z_t\\) variable. This type of prior is not conducive to building structured latent dynamics. But what can be done to tackle these problems?\nWhat we want is to replace the components of the VAE so it looks something like this,\n\nEncoder: \\(q_{\\phi}(Z|X) = q_{\\phi}(z_t|z_{1:t-1},x_{1:t})\\) (later when looking at DVAEs we will learn this isn’t quite right)\nDecoder: \\(p_{\\theta_x}(X|Z) = p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t})\\)\nPrior: \\(p(Z) = p_{\\theta_z}(z_t|z_{t-1})\\)\n\nWe could then design some simple prior dynamics and given an initial \\(z_0\\) use \\(p_{\\theta_z}(z_t|z_{t-1})\\) to generate any sequence of arbitrary length. To design a VAE that does this is known as a Dynamical VAE (DVAE) and we will see that it is just the VAE with sequential variables (\\(X\\) is replaced with \\(x_{1:T}\\); \\(Z\\) with \\(z_{1:T}\\))."
  },
  {
    "objectID": "posts/VAE/VAE.html#footnotes",
    "href": "posts/VAE/VAE.html#footnotes",
    "title": "Variational Autoencoders",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI took this example/analogy from a linear alegbra course by Nathan Kutz to describe dimension reduction for Principal Component Analysis. I however, altered and extended the analogy to suit my purposes for explaining VAEs (and ultimately DVAEs and discovering the latent dynamics high dimensional of systems)↩︎"
  }
]