[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Marco Star. I am pursuing a PhD in mechanical engineering, where I mainly focus on using deep learning for machinery prognostics. Basically, I try to construct deep neural networks to estimate the remaining useful life of machines. This blog exists as an exercise for me to write about general topics that interest me (deep learning, generative models, Bayesian filters, etc…). I decided to do this writing in blog form simply for the experience of creating a blog. Although if anyone reads this, I hope the content aids your understanding of these topics. Normally when I try learning something new, I end up with multiple tabs open in my browser, each with different explanations of the topic. Eventually, I end up developing my own way of understanding the material. I would be honoured if this blog ends up as one of your tabs."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Kalman Filters\n\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nMarco Star\n\n\n\n\n\n\n  \n\n\n\n\nDynamical Variational Autoencoders\n\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2022\n\n\nMarco Star\n\n\n\n\n\n\n  \n\n\n\n\nVariational Autoencoders\n\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2022\n\n\nMarco Star\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DVAE/DVAE.html",
    "href": "posts/DVAE/DVAE.html",
    "title": "Dynamical Variational Autoencoders",
    "section": "",
    "text": "This post is about the Dynamical Variational Autoencoder, which is like the Variational Autoencoder but for sequences of inputs such as time series. It is meant to be the sequel to my post about Variational Autoencoders"
  },
  {
    "objectID": "posts/DVAE/DVAE.html#introduction",
    "href": "posts/DVAE/DVAE.html#introduction",
    "title": "Dynamical Variational Autoencoders",
    "section": "Introduction",
    "text": "Introduction\nPreviously I have gone over Variational Autoencoders and mentioned that they are not suitable in their common form if we want to learn the underlying latent dynamics of a system. The main problem is that the generative model (the prior and decoder) only generates static data and isn’t concerned with the underlying dynamics of the data. For example, if we want to use the VAE to generate a sequence of positions of a moving object \\(X = [x_1,...,x_T]\\) then we would do the following,\n\nSample from the prior, \\(Z = [z_1,...,z_T] \\sim p(Z)\\)\nDecode the samples, \\(p_{\\theta_x}(X|Z)\\) and sample generated input data \\(X\\)\n\nNote, when I refer to the generative model I refer to the joint distribution between the input variables (\\(X\\)) and the latent variables (\\(Z\\)) i.e. \\(p(X,Z)\\). This can be written as \\(p(X,Z) = p(X|Z)p(Z)\\) which is just the decoder and prior of a VAE. Hence, when I refer to the generative model I am referring to these two components of the VAE which can practically be applied using the above steps.\nHere the generative model essentially defines all possible input position sequences \\(X=[x_1,...,x_T]\\). But what if we want to generate any arbitrary length e.g. we have some data and want to forecast possible future positions until they reach some target position. Or what if I want to understand or use the underlying dynamics behind these positions, so I can generate more accurate or plausible sequences. How would I do this?\nLuckily there are a type of VAE called the Dynamical Variational Autoencoder (DVAE) which solves this problem and improves the quality of generated sequential signals when compared to the regular VAE. The same underlying theory from the VAE is still there, in fact the underlying model is pretty much the same we just need to write it in a different mathematical form and we will get the DVAE. I’ll try to go through a lot of the basic theory in this post, but if you want a more in-depth resource on DVAEs, then I highly suggest the following review paper (Girin et al. 2021)."
  },
  {
    "objectID": "posts/DVAE/DVAE.html#theory",
    "href": "posts/DVAE/DVAE.html#theory",
    "title": "Dynamical Variational Autoencoders",
    "section": "Theory",
    "text": "Theory\nSo how do we implement a DVAE? Well lets first look at the components of a regular VAE,\n\nInference model: \\(q_{\\phi}(Z|X)\\)\nGenerative model: \\(p_{\\theta}(X,Z) = p_{\\theta_x}(X|Z) p_{\\theta_z}(Z)\\)\n\nWhere any subscripts on these probability distributions are the learnable parameters of the neural network that represents the distribution e.g. if \\(q_{\\phi}(Z|X)\\) is the neural network with inputs \\(X\\) and outputs \\(Z\\) then \\(\\phi\\) are the learnable parameters that will be optimised during training.\nNote, if you are familiar with VAEs you may wonder why the prior \\(p_{\\theta_z}(Z)\\) has parameters \\(\\theta_z\\) associated with it, as we normally define it as a standard normal distribution (so there is nothing to optimise). This is because in the DVAE formulation it will be a neural network, so it’s a teaser for later (you can ignore it for now).\nNow all we need to do is say that these variables are sequences of some arbitrary length \\(T\\) i.e. \\(X= x_{1:T} = [x_1,...,x_T]\\) and \\(Z = z_{1:T} =[z_1,...,z_T]\\) and substitute this in.\n\nInference model: \\(q_{\\phi}(z_{1:T}|x_{1:T})\\)\nGenerative model: \\(p_{\\theta}(x_{1:T},z_{1:T}) = p_{\\theta_x}(x_{1:T}|z_{1:T}) p_{\\theta_z}(z_{1:T})\\)\n\nOkay, you might be thinking “how has this helped”, well we can now rewrite these probability distributions as products of other probability distributions. Let’s first illustrate this for the generative model.\n\\[\n    \\text{Generative: } p_{\\theta}(x_{1:T}, z_{1:T}) = \\prod_{t=1}^T p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}) p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\n\\]\nNotice now we have this causal model, which is to say variables at time \\(t\\) only depend on past variables \\(1:t-1\\) (the present value is determined by the past values). This means our new decoder and prior models become,\n\\[\n    \\text{Decoder: } p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}) \\\\\n    \\text{Prior: } p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\n\\]\nThe prior generates the next latent variable \\(z_t\\) which can then be used to input all \\(z_{1:t}\\) into the decoder. We can alternate or chain this operation to generate an entire sequence of \\(x_{1:T}\\) for any arbitrary value of \\(T\\) by simply alternating between generating the next \\(z_t\\) value and then generate \\(x_t\\), which helps generate \\(z_{t+1}\\) which generates \\(x_{t+1}\\) and so on.\nBut to train the generative model we need an inference model to encode the inputs to a latent space. When breaking the inference model into a product of distributions we get,\n\\[\n    \\text{Inference: } q_{\\phi}(z_{1:T}|x_{1:T}) = \\prod_{t=1}^T q_{\\phi}(z_t|z_{1:t-1}, x_{1:T})\n\\]\nThis cannot be simplified further; unlike the joint distribution we are stuck with the fact we are given \\(x_{1:T}\\). Hence, this is a noncausal model where future values are needed to calculate present variables at time \\(t\\). Since the inference model is only used when training, \\(x_{1:T}\\) is anyway a given/available to us. This is because it is our training data, so it’s not a big deal that this model is noncausal (we are not cheating by looking into the future). But this does have an important impact on the model which may not be intuitive. As my gut instinct if I had to extend the VAE for sequential inputs would be to use \\(x_{1:t}\\) for the inference model. However, by breaking the probability distribution of the sequence into the product of distributions which represent the current values at time \\(t\\) we can see \\(x_{1:T}\\) is needed for the inference model. But how do we represent entire sequences (which could be any arbitrary length \\(T\\)) so that they can be used as inputs to a neural network?\nThe most common way is to use a Recurrent Neural Network (RNN). Many implementations will input a sequence (e.g. \\(x_{1:t}\\)) and let the hidden variable at that time \\(t\\) be a representation of the entire sequence i.e. \\(h_t := x_{1:t}\\). This can be seen in the diagram below where the RNN cell takes in the previous hidden state (\\(h_{t-1}\\)) and current input value (\\(x_t\\)) and outputs \\(h_t\\). Since each hidden state carries the information of the previous and current input, \\(x_{1:t}\\), the hidden state \\(h_t\\) is used to represent this input sequence. Note theoretically other models could be used to represent the input sequence \\(x_{1:t}\\), but the RNN is chosen due to its simplicity as it represents the entire sequence with just one variable \\(h_t\\). For \\(x_{1:T}\\) we can use a forward-backward RNN, where we essentially run the forward RNN (as shown in the diagram) and then run another backwards (from \\(t=T\\) to \\(t=1\\)) with \\(h_{T:1}\\) and \\(x_{T:1}\\) as the inputs to the RNN (we can concatenate them together in practice, so they are one input). We will use \\(\\overleftarrow{h}_t\\) as the symbol for hidden variable output of a forward-backwards RNN where, \\(\\overleftarrow{h}_t := x_{1:T}\\).\n\nWe now need to construct a network which will represent the inference distribution \\(q_{\\phi}(z_t|z_{1:t-1}, x_{1:T})\\) and the generative model distributions, i.e. the prior \\(p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\\) and the decoder \\(p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t})\\). This can be done by looking at the given values of each distribution and using those variables as the inputs to a neural network. The output of the neural network can be the mean and variance of a Gaussian distribution for simplicity. Hence, could construct the following networks as an attempt to model these distributions,\n\\[\n    \\text{inference network: } f_{\\phi}(z_{1:t-1}, x_{1:T}) = f_{\\phi}(z_{t-1}, \\overleftarrow{h}_t) = [\\mu_{\\hat{z}}, \\sigma_{\\hat{z}}]\n\\] \\[\n    \\text{prior network: } f_{\\theta_z}(z_{1:t-1}, x_{1:t-1}) = f_{\\theta_z}(z_{t-1}, h_{t}) = [\\mu_{z}, \\sigma_{z}]\n\\] \\[\n    \\text{decoder network: } f_{\\theta_x}(x_{1:t-1}, z_{1:t}) = f_{\\theta_x}(h_t, z_t) = [\\mu_{x}, \\sigma_{x}]\n\\]\nWhere each network outputs a mean \\(\\mu\\) and standard deviation \\(\\sigma\\) to define a Gaussian distribution \\(\\mathcal{N}(\\mu, \\sigma)\\), also note how each data sequence (\\(x_{1:t-1}\\)) was replaced by a hidden variable representation using a RNN (\\(x_{1:t-1} := h_{t}\\)). We also assumed for simplicity that the latent dynamics followed a Markov assumption so \\(z_t|z_{1:t-1}\\) became \\(z_t|z_{t-1}\\) i.e. the current variable only depends on the previous time variable. It should be mentioned that from a practical perspective the networks do not usually output the standard deviation but the log-variance (\\(\\text{log}\\sigma^2\\)) as the range is not restricted to only positive values, note you can make the standard deviation an output as long as an activation function is used to ensure the value is positive (e.g. softplus).\nAnother way of representing these equations and designing DVAEs is by using block diagrams. For example, you can see one of the inference model below.\n\nWith this diagram we can see how the data is handled. Each variable inside a circle is a stochastic variable, hence, if an arrow points to a circle we know that the output of the network that outputs that variable would be \\((\\mu, \\sigma)\\) for our example as we are using Gaussian distributions to model random variables. Rectangle blocks are deterministic variables and can therefore be direct outputs of a neural network (all the deterministic variables in the diagram are hidden variable outputs from a RNN). The diagram can make certain concepts clearer e.g. we can see why \\(h_t := x_{1:t-1}\\) as each \\(h_t\\) is generated from inputs \\(x_{t-1}\\) and \\(h_{t-1}\\), since \\(h_{t-1}:=x_{1:t-2}\\), \\(h_t\\) is a union of \\(x_{1:t-2}\\) and \\(x_{t-1}\\) which means it represents \\(x_{1:t-1}\\).\nThe diagram below is for the generative model.\n\nNotice that the generative model ultimately outputs \\(x_t\\) variables using the latent dynamics of \\(z_t\\). We can see this from the diagram through the arrows which point to \\(x_t\\) from \\(h_t\\) and \\(z_t\\) (meaning \\(h_t\\) and \\(z_t\\) are inputs). Since, \\(h_t := x_{1:t-1}\\) we have \\(f_{\\theta_x}(x_{1:t-1},z_t) = x_t\\) which is what we stated before as the decoder model. Hence, these diagrams can be a handy visual representation of the equations we stated previously, and sometimes I find it useful to draw the diagrams as they help me see how the data “flows” through the networks, and then from the diagrams see what my equations and probability distribution representations are.\nFinally, we should look at the loss function used to train the DVAE. Let us first have a look the loss function for a regular VAE i.e. the Evidence Lower Bound (ELBO),\n\\[\n\\text{ELBO} = \\mathbb{E}_{q_{\\phi}(Z|X)}\\left[\\text{log} p_{\\theta_x}(X|Z)\\right] - D_{KL}\\left( q_{\\phi}(Z|X) || p(Z)\\right)\n\\]\nWe can do what we did previously to define the DVAE from the VAE formulation, that is, make the following replacements, \\(X= x_{1:T} = [x_1,...,x_T]\\) and \\(Z = z_{1:T} =[z_1,...,z_T]\\) resulting in,\n\\[\n\\text{ELBO} = \\mathbb{E}_{q_{\\phi}(z_{1:T}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{1:T}|z_{1:T})\\right] - D_{KL}\\left( q_{\\phi}(z_{1:T}|x_{1:T}) || p_{\\theta_z}(z_{1:T})\\right)\n\\]\nFrom the previous analysis we know we can express the distribution of a sequence such as \\(x_{1:T}\\) with a product of distributions defining \\(x_t\\) given the previous \\(x_{1:t-1}\\). Since both loss terms have \\(log\\) acting on the distributions, the product over time period \\(T\\) can become a sum over \\(T\\). The final loss ends up being,\n\\[\n\\text{ELBO} = \\sum_{t=1}^{T} \\mathbb{E}_{q_{\\phi}(z_{1:t}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{t}|x_{1:t-1}, z_{1:t})\\right] - \\sum_{t=1}^T \\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})} \\left[D_{KL}\\left( q_{\\phi}(z_{t}|z_{1:t-1}, x_{1:T}) || p_{\\theta_z}(z_{t}| z_{1:t-1}, x_{1:t-1})\\right)\\right]\n\\]\nNote the expected value is with respect to the inference distribution. Hence, when you see a term like \\(\\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})}\\) is the loss function above, this means the sequence \\(z_{1:t-1}\\) should come from the inference model. For example, the KL-divergence has the term, \\(p_{\\theta_z}(z_{t}| z_{1:t-1}, x_{1:t-1})\\), inside the expection \\(\\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})}\\). This means we use the prior model to generate \\(z_t\\) but the previous latent variables \\(z_{1:t-1}\\) came from the inference model. Another example is the log-likelihood which is \\(\\mathbb{E}_{q_{\\phi}(z_{1:t}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{1:t}|x_{1:t-1}, z_{1:t})\\right]\\). Here we use the decoder \\(p_{\\theta_x}\\) to generate \\(x_t\\) using the previous \\(x_{1:t-1}\\) values and the latent variables again from the inference model as the expectation (\\(\\mathbb{E}\\)) is with respect to the inference distribution \\(q_{\\phi}(z_{1:t}|x_{1:T})\\). Again as a reminder, for a more detailed look at all this theory {% cite Girin2020 %} has an awesome review paper on the topic."
  },
  {
    "objectID": "posts/DVAE/DVAE.html#constructing-a-dvae",
    "href": "posts/DVAE/DVAE.html#constructing-a-dvae",
    "title": "Dynamical Variational Autoencoders",
    "section": "Constructing a DVAE",
    "text": "Constructing a DVAE\nNow lets look at an example of using the DVAE to model a dynamic system. I used the example of a mass-spring system in my post about VAEs, but stated we needed a way of handling sequences to allow the network to find the underlying dynamics of the system. Now that we have the DVAE in our tool kit, we have a generative model well suited to handle sequence inputs. As a reminder the diagram of the mass-spring system is below, where the mass is moving up and down in the z-direction. The idea here is that we pretend we have never seen a mass-spring system before in our lives, and so we set up sensors around the room to measure the distance between the sensor and the mass (these are labelled as \\(x\\) on the diagram). If we knew to set up the sensor in the z-direction we would have a nice 1D representation of the system dynamics, but we don’t know that, so instead we have a bunch of sensor signals which together give us a \\(n\\)-dimentional representation of the system dynamics. Our aim is to use the DVAE to find a lower dimensional representation of the system (it does not necessarily have to be the dynamics in the z-direction).\n\nWe can generate some of these sensor readings by simulating the motion of the system and placing \\(n\\) sensors randomly around a room by specifying their Cartesian coordinates,\n\\[\n    (x, y, z) \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\nHere \\(n=14\\) and we can plot what each sensor looks like as an example of the dynamics of 1 mass-spring system.\n\n\nCode\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sb \nimport torch \nimport torch.nn as nn \n\n%matplotlib inline \nsb.set_theme()\n\n\n\n\nCode\ndef sensor_loc(n):\n    # coordinates of sensors in a cartesian plane (origin is when z=0 before spring-mass system is stretched)\n    x_sen = np.random.normal(size=[n, 1]) \n    y_sen = np.random.normal(size=[n, 1])   \n    z_sen = np.random.normal(size=[n, 1])   \n    return (x_sen, y_sen, z_sen)\n\ndef get_sensor_values(sen_loc, t_min, t_max, res=100, m=1., k=1., z0=1.):\n    t = np.linspace(t_min, t_max, res)\n    z = z0*np.cos(np.sqrt(k/m) * t)     # spring mass motion along the z-axis \n\n    x_sen, y_sen, z_sen = sen_loc\n\n    z_diff = z - z_sen  # only difference in the z-axis\n    xs = np.sqrt(x_sen**2 + y_sen**2 + z_diff**2)\n\n    return t, xs.T     # shape (time, n)\n\nsensors = 14\nsen_loc = sensor_loc(sensors)\nt, X = get_sensor_values(sen_loc, 0, 20, res=200)\n\nplt.figure(figsize=(18,9))\nplt.plot(t, X)\nplt.show()\n\n\n\n\n\nTo train a DVAE we will need a whole dataset of mass-spring system dynamics. Hence, I will simulate 100 different mass-spring systems with slightly different mass, spring constant and initial values for training and another 100 for testing the model. From now on I will refer to each different mass-spring system as a “unit” (as its easier to type). Hence, we have 100 different training units and 100 testing units. For model training, I have also used a sliding window so inputs into the model are “packets” of the signal with a time window size \\(T=40\\). So inputs are tensors of size (batch size, T, sensors) while the testing inputs are still the entire time sequence. The training data is split into these time windowed packets, simply so we have more data that can be computed in parallel which allows for quicker training on a GPU. Each sensor signal is also normalised to have a mean of zero and a standard deviation of one. This helps with training the network as the signals don’t have varying magnitudes, so the network can stick to a known range.\n\n\nCode\ndef get_data(units, sensor_loc, t_min=0, t_max=20, res=200):\n    data = [] \n\n    for _ in range(units):\n        m = np.random.rand()\n        k = np.random.rand()\n        z0 = np.random.rand()\n\n        t, X = get_sensor_values(sen_loc=sensor_loc, t_min=t_min, t_max=t_max, res=res, m=m, k=k, z0=z0)   # generate data for different spring-mass systems \n\n        # Normalize data to improve training \n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n        data.append(torch.tensor(X))\n\n    data = torch.stack(data, dim=0)     # shape (units, time, No. of sensors) \n    return data \n\ntrain_units = 100\ntest_units = 100 \ntrain_data = get_data(train_units, sensor_loc=sen_loc)\ntest_data = get_data(test_units, sensor_loc=sen_loc)\n\n\n\n\nCode\nplt.figure(figsize=(18,9))\nplt.plot(train_data[0])\nplt.show()\n\n\n\n\n\n\n\nCode\ndef prep_data(data, T, bs):\n    data = data.unfold(1, T, 1).permute(0,1,3,2)                    # (n, units, T, dim)\n    data = torch.reshape(data, [-1,data.shape[-2],data.shape[-1]])  # (n*units, T, dim)\n\n    train_DataSet = torch.utils.data.TensorDataset(data, data)\n    train_loader = torch.utils.data.DataLoader(train_DataSet, shuffle=True, batch_size=bs)\n    return train_loader \n\nT = 40\nbs = 150 \ntrain_loader = prep_data(train_data, 40, bs)\n\n\nHere I simply have some code that defines some utilities like a simple Feedforward or Multilayer Perceptron (MLP) neural network as well as some loss functions needed in the DVAE i.e. the KL divergence between two Gaussian distributions and the log-likelihood of a Gaussian distribution.\n\n\nCode\n# -------------------\n# Neural Network Utils\n# -------------------\nclass MLP(nn.Module):\n  def __init__(self, input_dim, hidden_dim, output_dim):\n    super(MLP, self).__init__()\n    self.linear1 = nn.Linear(input_dim, hidden_dim)\n    self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n    self.linear3 = nn.Linear(hidden_dim, output_dim, bias=None)\n\n    self.nonlinearity = torch.relu\n\n  def forward(self, x):\n    h = self.nonlinearity( self.linear1(x) )\n    h = h + self.nonlinearity( self.linear2(h) )\n    return self.linear3(h)\n\n# --------------\n# Loss \n# --------------\ndef loss_KLD(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    KL-divergance between 2 Gaussian distributions, given the mean and log-variance of \n    Gaussian 1 and Gaussian 2. \n    \"\"\"\n    loss = -0.5 * torch.sum(logvar1 - logvar2\n    - torch.div(logvar1.exp() + (mean1 - mean2) ** 2, logvar2.exp() + 1e-10))\n    return loss \n\ndef loglike(x, mean, lvar):\n    var = torch.exp(lvar) \n    return -0.5 * torch.log(2*(np.pi)*var) - (x - mean) ** 2 / (2*var)\n\n\nThe following is code for the actual DVAE itself.\n\n\nCode\nclass DVAE(nn.Module):\n    def __init__(self, xdim, zdim, hdim):\n        super().__init__()\n        self.xdim = xdim \n        self.zdim = zdim \n        self.hdim = hdim \n\n        # deterministic (RNNs)\n        self.x_fwd = nn.GRU(xdim, hdim, batch_first=True)\n        self.x_bck = nn.GRU(xdim+hdim, hdim, batch_first=True)\n\n        # latent \n        self.z_gen = MLP(hdim+zdim, hdim*2, hdim) # prior\n        self.z_inf = nn.GRU(hdim, zdim*2, batch_first=True) # inference model \n\n        self.prior_mean = MLP(hdim, hdim, zdim)\n        self.prior_lvar = MLP(hdim, hdim, zdim)\n        \n        self.inf_mean = MLP(hdim, hdim, zdim)\n        self.inf_lvar = MLP(hdim, hdim, zdim)\n\n        # generative \n        self.x_dec = MLP(zdim+hdim, hdim, xdim*2)   # decoder\n\n    # --- Helper functions --- \n    def sample(self, mean, lvar):\n        std = torch.exp(0.5 * lvar)\n        eps = torch.randn(mean.size()).to(mean.device)\n        return mean + eps * std \n\n    def get_stats(self, stats):\n        dim = stats.shape[-1] // 2 \n        mean = stats[...,:dim]\n        lvar = stats[...,dim:]\n        return mean, lvar \n\n    ## --- Used for Training --- \n    def decode(self, hs, zs):\n        hzs = torch.cat((hs, zs), dim=-1)\n        x_stats = self.x_dec(hzs)\n\n        x_mean, x_lvar = self.get_stats(x_stats)\n        return x_mean, x_lvar \n\n    def represent_x(self, xs):\n        bs, _, xdim = xs.shape\n\n        x0 = torch.zeros(bs, 1, xdim).to(xs.device) # dummy init. var.\n        x_tm1 = torch.cat((x0, xs[:,:-1,:]), 1) # tm1 = current time minus 1 = previous time variable \n        hs, _ = self.x_fwd(x_tm1)\n        return hs\n\n    def inference(self, xs):\n        # --- Encode inputs to represent sequences x_{1:T} ---\n        self.hs = self.represent_x(xs)\n\n        h_x = torch.cat((self.hs, xs), dim=-1)\n        g_revs, _ = self.x_bck(torch.flip(h_x, [1]))    # backward RNN \n        gs = torch.flip(g_revs, [1]) \n\n        # --- Latent ---\n        z_stats, _ = self.z_inf(gs)\n        z_means, z_lvars = self.get_stats(z_stats)\n        zs = self.sample(z_means, z_lvars)\n        \n        return zs, z_means, z_lvars\n\n    def generate_z(self, hs, z_tm1):\n        hzs = torch.cat((hs, z_tm1), dim=-1)\n        hzs = self.z_gen(hzs)\n\n        z_means = self.prior_mean(hzs)\n        z_lvars = self.prior_lvar(hzs)\n        return z_means, z_lvars\n\n    def generate_x(self, zs, use_pred=1):\n        bs, seq, _ = zs.shape\n\n        x_tm1 = torch.zeros(bs, 1, self.xdim).to(zs.device)    # dummy init. var. \n        h_tm1 = torch.zeros(1, bs, self.hdim).to(zs.device)\n\n        x_means = []\n        x_lvars = [] \n        hs = []\n\n        for t in range(seq):\n            h, h_tm1 = self.x_fwd(x_tm1, h_tm1)   # h.size = (bs, 1, hdim), h_tm1.size = (1, bs, hdim)\n\n            # during training randomly replace x_{1:t} estimates with ground truth\n            random_num = np.random.uniform()\n            if random_num &lt;= use_pred:\n                h = h # use prediction\n            else:\n                h = self.hs[:,t,:].unsqueeze(1) # use ground truth\n\n            # estimate x \n            x_mean, x_lvar = self.decode(h, zs[:,t,:].unsqueeze(1))   # size (bs, 1, xdim)\n            x_tm1 = self.sample(x_mean, x_lvar)\n\n            x_means.append(x_mean)\n            x_lvars.append(x_lvar)\n            hs.append(h)\n        \n        x_means = torch.cat(x_means, dim=1)\n        x_lvars = torch.cat(x_lvars, dim=1)\n        hs = torch.cat(hs, dim=1)\n\n        return x_means, x_lvars, hs \n\n    def forward(self, xs, use_pred=1):\n        bs = xs.shape[0]\n\n        # generate latent sequence from training data using the inference model \n        zs_inf, zs_inf_mean, zs_inf_lvar = self.inference(xs)\n\n        # use latent sequence from inference model to estimate x-values \n        x_mean, x_lvar, hs = self.generate_x(zs_inf, use_pred=use_pred)\n\n        # generate prior z-values \n        z0 = torch.zeros(bs, 1, self.zdim).to(xs.device)\n        z_tm1 = torch.cat((z0, zs_inf[:,:-1,:]), dim=1)\n        zs_gen_mean, zs_gen_lvar = self.generate_z(hs, z_tm1)\n\n        # KL-div between inference and prior z-values \n        kl = loss_KLD(zs_gen_mean, zs_gen_lvar, zs_inf_mean, zs_inf_lvar) / bs \n\n        return kl, x_mean, x_lvar \n\n    def get_loss(self, xs, use_pred=1):\n        kl, x_mean, x_lvar = self.forward(xs, use_pred=use_pred)\n        nll = -loglike(xs, x_mean, x_lvar).mean()\n        return kl, nll\n    \n    ## --- Used for Testing ---\n    def generate(self, xs, return_hidden=False):\n        '''\n        Used to test reconstructions of xs given xs itself \n        '''\n        bs, seq, xdim = xs.shape\n\n        # --- Encode inputs to represent sequences x_{1:T} ---\n        x0 = torch.zeros([bs, 1, xdim]).to(xs.device)\n        x_tm1 = torch.cat((x0, xs[:,:-1,:]), dim=-2)  # x_{t-1} outputs h_t\n        hs, _ = self.x_fwd(x_tm1)     # forward RNN outputs h_{1:T} given x_{0:T-1}\n\n        # --- Storage --- \n        z_means = [] \n        z_lvars = [] \n        zs = [] \n        \n        # --- Latent ---\n        z_tm1 = torch.zeros(bs, self.zdim).to(xs.device)\n        for t in range(seq):\n            hzs = torch.cat((hs[:,t,:], z_tm1), dim=-1) # (bs, hdim+zdim)\n            hzs = self.z_gen(hzs)\n\n            z_mean = self.prior_mean(hzs)\n            z_lvar = self.prior_lvar(hzs)\n            z = self.sample(z_mean, z_lvar)\n\n            z_means.append(z_mean)\n            z_lvars.append(z_lvar)\n            zs.append(z)\n\n        z_means = torch.stack(z_means, dim=1)\n        z_lvars = torch.stack(z_lvars, dim=1)\n        zs = torch.stack(zs, dim=1)\n\n        # --- Generate/Decode --- \n        x_mean, x_lvar = self.decode(hs, zs)\n\n        if return_hidden:\n            return x_mean, x_lvar, z_means, z_lvars, hs\n\n        return x_mean, x_lvar, z_means, z_lvars\n\n    def generate_rec(self, x0, h0, z0_mean, z0_lvar):\n        '''\n        generates given time = t-1 values generate values at time = t \n        '''\n        # generate values reccurently i.e. given values @ t-1 generates values @ t \n        h1, h = self.x_fwd(x0, h0)\n        z0 = self.sample(z0_mean, z0_lvar)\n        hz = torch.cat((h1, z0), dim=-1)\n\n        hz = self.z_gen(hz)\n        z_mean = self.prior_mean(hz)\n        z_lvar = self.prior_lvar(hz)\n        z1 = self.sample(z_mean, z_lvar)\n\n        x_mean, x_lvar = self.decode(h1, z1)\n\n        return x_mean, x_lvar, z_mean, z_lvar, h\n\n    def overshoot(self, xs, split=None):\n        \"\"\"\n        Performs sequential overshooting. \n        i.e. randomly chooses a cut-off point in the data sequence xs and splits \n        it into a known and unknown sequence. Then uses the known sequence to extrapolate\n        the rest of the unknown sequence and compares with the true result. \n        \n        e.g. \n        xs = [x1, x2, x3, x4, x5, x6]\n        known_xs = [x1, x2]\n        unknown_xs = [x3, x4, x5, x6]\n\n        generate_known_xs = [x1_gen, x2_gen]\n\n        using x2_gen as the initial condition \n        extrapolated_xs = [x3_gen, x4_gen, x5_gen, x6_gen]\n\n        xs_gen = [x1_gen, x2_gen, x3_gen, x4_gen, x5_gen, x6_gen]\n\n        overshoot_loss = loglike(xs, xs_gen_mean, xs_gen_lvar)\n        \"\"\"\n        seq = xs.shape[1]\n        if split == None:\n            percent = np.random.uniform(0.1)\n            known_seq = int(seq * percent)\n        else:\n            known_seq = int(seq * split)\n        T = seq - known_seq    # extrapolation length \n\n        # storing \n        xs_means = []\n        xs_lvars = [] \n        zs_means = [] \n        zs_lvars = [] \n        def store(x_mean, x_lvar, z_mean, z_lvar):\n            xs_means.append(x_mean)\n            xs_lvars.append(x_lvar)\n            zs_means.append(z_mean)\n            zs_lvars.append(z_lvar)\n            return xs_means, xs_lvars, zs_means, zs_lvars\n        \n        # generating and extrapolating values \n        obs = xs[:,:known_seq,:]\n        x_mean, x_lvar, z_mean, z_lvar, hs = self.generate(obs, return_hidden=True)\n        xs_means, xs_lvars, zs_means, zs_lvars = store(x_mean, x_lvar, z_mean, z_lvar)\n\n        x = self.sample(x_mean, x_lvar)\n        x = x[:,-1:,:]\n        h = hs[:,-1,:].unsqueeze(0) \n        z_mean = z_mean[:,-1:,:]\n        z_lvar = z_lvar[:,-1:,:]\n        for t in range(T):\n            x_mean, x_lvar, z_mean, z_lvar, h = self.generate_rec(x, h, z_mean, z_lvar)\n            xs_means, xs_lvars, zs_means, zs_lvars = store(x_mean, x_lvar, z_mean, z_lvar)\n\n            x = self.sample(x_mean, x_lvar)\n\n        xs_means = torch.cat(xs_means, dim=1)\n        xs_lvars = torch.cat(xs_lvars, dim=1)\n        zs_means = torch.cat(zs_means, dim=1)\n        zs_lvars = torch.cat(zs_lvars, dim=1)\n\n        return xs_means, xs_lvars, zs_means, zs_lvars\n\n\n\nNow that we have defined all the components we can train the model.\n\n\nCode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nxdim = sensors \nhdim = 50 \nzdim = 1\n\nmodel = DVAE(xdim, zdim, hdim).to(device)\n\nlr = 1e-3 \noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n\n\n\nCode\ndef train_model(model, optimizer, train_loader, epochs, use_pred=1):\n    for epoch in range(1, epochs+1):\n        for xs, _ in train_loader:\n            xs = xs.float().to(device)\n\n            kl, nll = model.get_loss(xs, use_pred)\n\n            elbo = nll + kl\n\n            elbo.backward()\n            optimizer.step()\n            optimizer.zero_grad() \n\n        if epoch == 1 or (epoch % 10) == 0: \n            print(\"Epoch: {}/{}, elbo: {:.4f}, nll: {:.4f}, kl: {:.4f}\"\n            .format(epoch, epochs, elbo, nll, kl))\n\n\n\n\nCode\ntrain_model(model, optimizer, train_loader, epochs=100, use_pred=0.5)\n\n\nEpoch: 1/100, elbo: 20.8290, nll: 0.8044, kl: 20.0246\nEpoch: 10/100, elbo: 20.5449, nll: 0.5424, kl: 20.0025\nEpoch: 20/100, elbo: 19.2262, nll: -0.7744, kl: 20.0007\nEpoch: 30/100, elbo: 20.4248, nll: 0.4242, kl: 20.0005\nEpoch: 40/100, elbo: 19.9360, nll: -0.0643, kl: 20.0003\nEpoch: 50/100, elbo: 19.3641, nll: -0.6360, kl: 20.0002\nEpoch: 60/100, elbo: 18.6368, nll: -1.3633, kl: 20.0001\nEpoch: 70/100, elbo: 18.8004, nll: -1.2230, kl: 20.0234\nEpoch: 80/100, elbo: 19.8288, nll: -0.1713, kl: 20.0001\nEpoch: 90/100, elbo: 18.4936, nll: -1.5064, kl: 20.0000\nEpoch: 100/100, elbo: 18.1595, nll: -1.8405, kl: 20.0000\n\n\nHere we will test the DVAE by first seeing if the generative model can simply reconstruct the sensor signals given. Essentially we will give it the true \\(x_{t-1}\\) value and it will generate \\(z_t\\), which will be decoded to \\(x_t\\). This will be done for all \\(t \\in [0,T]\\) where \\(T\\) is the entire sequence (instead of just a time window like in training). Since \\(x_0\\) is a dummy variable I will ignore it when plotting the sensors.\n\n\nCode\ndef test_model(model, test_data):\n    results = {\n        \"x_true\": [], \n        \"x_mean\": [],\n        \"x_stds\": [],\n        \"z_mean\": [],\n        \"z_stds\": [], \n        \"rmse\": [],\n    }\n\n    for xs in test_data:\n        xs = xs.unsqueeze(0).float().to(device)\n        \n        x_mean, x_lvar, z_mean, z_lvar = model.generate(xs)\n        \n        RMSE = ((x_mean - xs) ** 2).mean()\n        x_stds = torch.exp(0.5 * x_lvar)\n        z_stds = torch.exp(0.5 * z_lvar)\n        \n        results[\"x_true\"].append(xs.detach().cpu().numpy())\n        results[\"x_mean\"].append(x_mean.detach().cpu().numpy())\n        results[\"x_stds\"].append(x_stds.detach().cpu().numpy())\n        results[\"z_mean\"].append(z_mean.detach().cpu().numpy())\n        results[\"z_stds\"].append(z_stds.detach().cpu().numpy())\n        results[\"rmse\"].append(RMSE.detach().cpu().numpy())\n\n    return results \n\n\n\n\nCode\nresults = test_model(model, test_data)\nprint(\"AVG RMSE: \", sum(results[\"rmse\"]) / len(results[\"rmse\"]))\n\n\nAVG RMSE:  0.039877471905201675\n\n\nI only plot one sensor below otherwise I find it gets to cluttered and difficult to see how well it performed.\n\n\nCode\nunit = 11 \n# since x0 = tensor of zeros the first value can be quite off, but once real data comes in we can see it tracks the signal nicely \nx_true = results[\"x_true\"][unit-1][0,1:,:]\nx_mean = results[\"x_mean\"][unit-1][0,1:,:]\nz_mean = results[\"z_mean\"][unit-1][0,1:,:]\nz_stds = results[\"z_stds\"][unit-1][0,1:,:]\n\ns = 10   # sensor number \nplt.figure(figsize=(18,9))\nplt.plot(x_true[...,s-1])\nplt.plot(x_mean[...,s-1])\nplt.show()\n\n\n\n\n\nWe can also plot the latent variable sequences we generated when we reconstructed all these sensors.\n\n\nCode\nplt.figure(figsize=(18,9))\nplt.plot(z_mean)\nplt.show()\n\n\n\n\n\nPlotting all the latent variable sequences we get.\n\n\nCode\n#collapse-hide\nplt.figure(figsize=(18,9))\nfor i in range(100):\n    plt.plot(results[\"z_mean\"][i][0,1:,:])\nplt.show()\n\n\n\n\n\nYou could imagine we could interpolate a new latent sequence in this latent space using our own initial condition and the prior latent dynamics. If we did this and decoded it back to the input space we should get new reconstructed sensor values of some fictitious mass-spring system that doesn’t exist (I’m aware none of these systems existed as I simulated them, but if this was all done through real life experiments I could simulate a new realistic looking virtual experiment). This could be applied in interesting applications such as generating audio, where we use real audio signals to train the model and then generate new audio using the trained model. We can also use a conditional DVAE where we generate new inputs conditioned on other variables. This can be used in machinery prognostics where you might want to find the Remaining Useful Life (RUL) of a machine or component. The RUL could be the generated inputs and these can be conditioned on sensor values. This would change the DVAE model to,\n\\[\n    \\text{Encoder/Inference: } q_{\\phi}(z_t|z_{1:t-1}, x_{1:T}, u_{1:T})\n\\] \\[\n    \\text{Decoder: } p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}, u_{1:t})\n\\] \\[\n    \\text{Prior: } p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1}, u_{1:t})\n\\]\nwhere in this example \\(x_t = (RUL)_t\\) and $u_t = $ sensor signals at time \\(t\\).\nThe code below shows how one would generate new signals from “virtual” mass-spring systems that were not seen during training.\n\n\nCode\ndef generate_sensors(model, xs, split, device):\n    results = {\n        \"x_mean\": [], \n        \"x_stds\": [],\n        \"z_mean\": [],\n        \"z_stds\": []\n    }    \n    xs = xs.unsqueeze(0).float().to(device)\n    def add_to_results(results, x_mean, x_lvar, z_mean, z_lvar):\n        x_stds = torch.exp(0.5 * x_lvar)\n        z_stds = torch.exp(0.5 * z_lvar)\n\n        results[\"x_mean\"].append(x_mean.detach().cpu())\n        results[\"x_stds\"].append(x_stds.detach().cpu())\n        results[\"z_mean\"].append(z_mean.detach().cpu())\n        results[\"z_stds\"].append(z_stds.detach().cpu())\n        return results \n\n    x_mean, x_lvar, z_mean, z_lvar = model.overshoot(xs, split)\n    results = add_to_results(results, x_mean, x_lvar, z_mean, z_lvar)\n\n    results[\"x_mean\"] = torch.cat(results[\"x_mean\"], dim=1).numpy()\n    results[\"x_stds\"] = torch.cat(results[\"x_stds\"], dim=1).numpy()\n    results[\"z_mean\"] = torch.cat(results[\"z_mean\"], dim=1).numpy()\n    results[\"z_stds\"] = torch.cat(results[\"z_stds\"], dim=1).numpy()\n\n    return results \n\nunit = 10\nsplit = 0.7\nseq = test_data[unit-1].shape[0]\n\ngen_results = generate_sensors(model, test_data[unit-1], split, device=device)\n\n\nHere we give our model 70% of the sensors and ask it to extrapolate the 30% we didn’t give it. We can plot the results below.\n\n\nCode\nz_mean = gen_results[\"z_mean\"][0,1:,0]\nt = torch.linspace(0, z_mean.shape[0], z_mean.shape[0])\n\nplt.figure(figsize=(18,9))\nplt.plot(t, z_mean)\nplt.show()\n\n\n\n\n\n\n\nCode\ns = 7 # sensor number \nx_mean = gen_results[\"x_mean\"][0,1:,s]\nx_stds = gen_results[\"x_stds\"][0,1:,s]\n\nplt.figure(figsize=(18,9))\nplt.plot(t, x_mean, label=\"estimate\")\nplt.plot(t, test_data[unit-1][1:,s], label=\"true\")\nplt.fill_between(t, x_mean + 3*x_stds, x_mean - 3*x_stds, alpha=0.3)\nplt.legend()\nplt.show()\n\n\n\n\n\nThis doesn’t look too bad, but each time we are sampling a different \\(z_t\\) value at each time and decoding it. Hence, each time we call the function to extrapolate we generate a different trajectory \\(z_{1:T}\\) and decode to \\(x_{1:T}\\). So lets run the same code again and get an idea of how much the extrapolated results can vary.\n\n\nCode\nunit = 10\nsplit = 0.7\nseq = test_data[unit-1].shape[0]\n\ngen_results = generate_sensors(model, test_data[unit-1], split, device=device)\n\n\n\n\nCode\ns = 1\n\nplt.figure(figsize=(18,9))\nplt.plot(gen_results[\"x_mean\"][0,1:,s], label=\"estimate\")\nplt.plot(test_data[unit-1][1:,s], label=\"true\")\nplt.legend()\nplt.show()\n\n\n\n\n\nAs you can see the trajectory still maintains the oscillating dynamics but can have a lot of variation when extrapolating. But we can generate signals which look somewhat like the original signal in the sense certain characteristics (such as the oscillations) are captured by the underlying prior latent dynamics. The latent dynamics could be smoother if we played around with the prior model. Here we just used a standard feedforward network which took \\(z_{t-1}\\) to \\(z_{t}\\). To improve the results perhaps a better prior model could be used with more structured dynamics. But hopefully the theory and the simple example (relatively speaking) helps you if you want to use these for an application. So we at least finish this post with an idea of how to improve the model and a cool way of generating dynamic data.\nI think this is a good place to stop. Ultimately the aim of these posts is to build up to the point where we are using DVAEs to estimate the RUL of machinery based on input sensor signals. So the theory and basic examples shown here is to get everyone comfortable with the basic ideas behind DVAEs before we move on to more practical applications. Hopefully this post has aided your understanding of DVAEs (and/or introduced you to a cool new technique), I know the mass-spring system analogy really helped me think about how DVAEs are VAEs but extended to deal with dynamic systems or sequential data."
  },
  {
    "objectID": "posts/VAE/VAE.html",
    "href": "posts/VAE/VAE.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Variational Autoencoders (VAEs) have been covered in many different texts and blog posts. The goal here is not so much to repeat the information covered by so many others but instead to look at the VAE with the goal of introducing Dynamical Variational Autoencoders (DVAEs) later. Ultimately the aim is to cover how learn latent dynamics that describe the behaviour of complex systems. *Note, on the off chance someone is actually reading this (thank you for your interest) but this is my first blog post and so this content may change as I experiment with features and use this notebook as a prototype for future posts. So consider this a draft post for now"
  },
  {
    "objectID": "posts/VAE/VAE.html#a-simple-example",
    "href": "posts/VAE/VAE.html#a-simple-example",
    "title": "Variational Autoencoders",
    "section": "A Simple Example",
    "text": "A Simple Example\nA benefit of VAEs (Kingma and Welling 2014) is that they can take input values (\\(X\\)) and map them to a lower dimensional latent variable (\\(Z\\)). We will look at an example to show this more clearly; it will also be used in a later post to show how to model latent dynamics of systems with a lot of input variables 1. The example shown below is a mass that is suspended from the ceiling. Imagine I pull it in the \\(z\\)-direction and let go. You probably imagine the mass will bob up and down but imagine we don’t know that. Instead, we are alien scientists trying to uncover the hidden dynamics of this complex system we have never seen before. So we set up \\(n\\) cameras or motion sensors all around the room (where \\(n\\) is just some arbitrary number). Each sensor provides a different axis or perspective describing the motion of the spring-mass system.\n\nLet us formally define inputs \\(X = [x_1, x_2, x_3, ..., x_n]\\) which are our sensor readings. In fact, to get an idea of the complexity of the dynamics let us plot what some of these sensor readings may look like. We will say the spring mass system is modelled as,\n\\[\n    z(t) = z_0 \\text{cos}\\left(\\sqrt{\\frac{k}{m}} t\\right)\n\\] where we will simply assume, \\(z_0=k=m=1\\) (initial position, spring constant and mass equal 1). For plotting what the sensor reading may look like, we will assume that the sensors are randomly located around the spring-mass system. The sensors will return the distance between the mass and the sensor as it is in motion. Hence, we will randomly define the Cartesian coordinates of the sensors in the room and simply take the distance between the moving spring-mass system and the sensor locations as the sensor outputs.\n\n\nCode\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sb \n\n%matplotlib inline \nsb.set_theme()\n\n\n\n\nCode\ndef get_sensor_values(n, t_min, t_max, res=100, m=1., k=1., z0=1.):\n    t = np.linspace(t_min, t_max, res)\n    z = z0*np.cos(np.sqrt(k/m) * t)     # spring mass motion along the z-axis \n\n    # coordinates of sensors in a cartesian plane (origin is when z=0 before spring-mass system is stretched)\n    x_sen = np.random.normal(size=[n, 1]) \n    y_sen = np.random.normal(size=[n, 1])   \n    z_sen = np.random.normal(size=[n, 1])   \n\n    z_diff = z - z_sen  # only difference in the z-axis\n    xs = np.sqrt(x_sen**2 + y_sen**2 + z_diff**2)\n\n    return t, xs.T     # shape (time, n)\n\nt, X = get_sensor_values(10, 0, 100, res=1000)\n\nplt.figure(figsize=(18,9))\nplt.plot(t, X)\nplt.show()\n\n\n\n\n\nWe get some relatively weird looking data at least compared to the simple oscillating motion we would get if we only looked at the z-axis motion. The aim of the VAE is to find that simple oscillating motion from the data collected by all these sensors. With this simple system that we can compress all the input data \\(X\\) into a more simple latent space \\(Z\\). In this latent space the motion isn’t as complex and only requires 1 axis to fully describe the motion. The idea of the VAE is to represent high dimensional data in a representative latent space \\(Z\\) which tries to efficiently encode the input data into a lower dimensional space (in this case the latent space \\(Z\\) would fully be able to describe the motion of the system). But how does the VAE do this?"
  },
  {
    "objectID": "posts/VAE/VAE.html#the-components-of-the-vae",
    "href": "posts/VAE/VAE.html#the-components-of-the-vae",
    "title": "Variational Autoencoders",
    "section": "The Components of the VAE",
    "text": "The Components of the VAE\nThe VAE is a generative model. This means instead of simply learning an input \\(\\rightarrow\\) output relationship e.g. \\(f(X) = Z\\) or using probability distributions \\(p(Z|X)\\), we learn the joint probability between the inputs and latent variables \\(p(X,Z)\\). This allows us to transfer from one space to another as \\(p(X,Z) = p(Z|X)p(X)=p(X|Z)p(Z)\\) it also lets us generate inputs \\(X\\) simply by sampling the latent space \\(Z\\) and using \\(p(X|Z)\\) to generate new inputs \\(X\\). But how does the VAE do this?\nFirstly, if the goal is to model the joint distribution \\(p(X,Z)\\) which can be described using \\(p(X,Z) = p(Z|X)p(X)=p(X|Z)p(Z)\\), then since this is deep learning, let us model what we can using neural networks. We then have,\n\n\\(p(Z|X) = q_{\\phi}(Z|X)\\) is the encoder network as it encodes inputs to the latent space. \\(\\phi\\) are the parameters of the network and in most cases it models a Gaussian/Normal distribution by making the network output a mean and log-variance i.e. \\(q_{\\phi}(Z|X) = [\\mu_z, \\text{log}\\sigma_z^2]\\)\n\\(p(X|Z) = p_{\\theta_x}(X|Z)\\) is the decoder as it takes latent variables and transforms them back into the input space. The network parameters are \\(\\theta_x\\) and it models a Gaussian distribution like the encoder\n\\(p(Z) = p_{\\theta_z}(Z)\\) for VAEs this isn’t usually a network but instead we define it as a standard normal distribution \\(\\mathcal{N}(0,I)\\). The idea behind this is that we constain the latent space to this domain so after training we don’t need the encoder to tell us where the latent variables exist. Instead, we just sample from the defined distribution \\(Z \\sim \\mathcal{N}(0,I)\\) and decode to the input space to generate new input data.\n\\(p(X)\\) this is the distribution of the input data, we don’t know what this is and that is the motivation for trying our very best to get rid of it in any formula we see (out of sight out of mind). We will use all sorts of mathematical tricks to derive a loss function which does not include this term.\n\nTo train the VAE first notice that the encoding distribution can be expressed in the following form.\n\\[\n    p(Z|X) = \\frac{p(X|Z)p(Z)}{p(X)}\n\\]\nSince we don’t know what \\(p(X)\\) is we use variational techniques to estimate the unknown distribution \\(p(Z|X)\\) using a simpler distribution (usually a Gaussian distribution). Hence, we have,\n\\[\n    p(Z|X) \\approx q_{\\phi}(Z|X) = [\\mu_z, \\text{log}\\sigma_z^2] \\rightarrow \\mathcal{N}(\\mu_z, \\sigma_z^2)\n\\]\nTo derive the loss (known as the Evidence Lower Bound or ELBO) we try to minimise the difference between the simple distribution \\(q_\\phi(Z|X)\\) and the complex unknown \\(p(Z|X)\\). i.e. \\[\n    D_{KL}(q_\\phi(Z|X)||p(Z|X))\n\\] Where \\(D_{KL}\\) is the KL-divergance.\nThere are many resources which derive the ELBO, so I won’t go over the mathematical details. I will however show the final form of the ELBO and discuss the logic behind each term so that it may become clear how we can later leverage this to learn the latent dynamics of the system (when looking at DVAEs).\n\\[\n    \\text{ELBO} = \\mathbb{E}_{q_{\\phi}(Z|X)}\\left[\\text{log} p_{\\theta_x}(X|Z)\\right] - D_{KL}\\left( q_{\\phi}(Z|X) || p(Z)\\right)\n\\]\nTo optimise this we maximise the ELBO (or we minimise the -ELBO) loss by tuning the parameters network parameters \\(\\phi\\) and \\(\\theta_x\\). Let’s look at the first term, \\[\n    \\mathbb{E}_{q_{\\phi}(Z|X)}\\left[\\text{log} p_{\\theta_x}(X|Z)\\right]\n\\]\nThis is the log-likelihood which determines the likelihood of the decoded latent variable \\(Z\\) to match the target input \\(X\\). This is what we might think of when we think of a traditional or basic loss function; is the output of our network close to the target output we expect. The more interesting term here is the second one,\n\\[\n    D_{KL}\\left( q_{\\phi}(Z|X) || p(Z)\\right)\n\\]\nThis term optimises \\(q_{\\phi}(Z|X)\\) to encode the latent variables \\(Z\\) into a defined space \\(p(Z)\\) (for VAEs this is commonly described using the standard normal distribution). This means an optimised VAE has a latent space constrained to the prior distribution \\(p(Z)\\). Hence, we could theoretically design a simple prior (this is what we do by picking a standard normal distribution) and simplify the latent space, so it is easy to interpret."
  },
  {
    "objectID": "posts/VAE/VAE.html#problems",
    "href": "posts/VAE/VAE.html#problems",
    "title": "Variational Autoencoders",
    "section": "Problems",
    "text": "Problems\nFor learning the underlying dynamics of sequential input data the VAE is not the ideal model. One problem we have is that \\(Z\\) and \\(X\\) are fixed lengths. If we want to learn the dynamics we don’t it is not very useful if we can only deal with sequences of a certain arbitrary length that we choose to train with. e.g. if we trained with input time-series sequences of length \\(T\\), \\(X = [x_1,...,x_T]\\), then we can only reconstruct other \\(X\\) variables of the same length. Hence, if \\(X\\) denotes a list of positions then we are only outputting possible positions the spring-mass system could be in and not understanding the physics or dynamics behind how those positions came about. So VAEs can handle “static” cases where the data is a fixed length or size (e.g. generating images). However, if we want to learn the dynamics of the system we need to be able to handle the dynamic case. In this dynamic case the data could have different sequence lengths, but even if it isn’t, we don’t want to just learn all the possible positions of the spring-mass system, we also want to understand the underlying dynamics that determine how those positions change over time. Another problem is the prior of the VAE is generally a Gaussian distribution with zero mean and unit variance (standard Normal distribution). This is not ideal if we want to learn a sequence as the latent space has no sequential structure. If we use a standard Normal distribution then we are trying to construct a latent sequence which consists of sequences of randomly generated variables that have no relation to each other with respect to time i.e. \n\\[\n    z_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\]\nHence, the latent space consists of sequences \\(\\{z_t\\}_{t=0}^T\\) where each \\(z_t\\) was randomly generated and is not related to any other \\(z_t\\) variable. This type of prior is not conducive to building structured latent dynamics. But what can be done to tackle these problems?\nWhat we want is to replace the components of the VAE so it looks something like this,\n\nEncoder: \\(q_{\\phi}(Z|X) = q_{\\phi}(z_t|z_{1:t-1},x_{1:t})\\) (later when looking at DVAEs we will learn this isn’t quite right)\nDecoder: \\(p_{\\theta_x}(X|Z) = p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t})\\)\nPrior: \\(p(Z) = p_{\\theta_z}(z_t|z_{t-1})\\)\n\nWe could then design some simple prior dynamics and given an initial \\(z_0\\) use \\(p_{\\theta_z}(z_t|z_{t-1})\\) to generate any sequence of arbitrary length. To design a VAE that does this is known as a Dynamical VAE (DVAE) and we will see that it is just the VAE with sequential variables (\\(X\\) is replaced with \\(x_{1:T}\\); \\(Z\\) with \\(z_{1:T}\\))."
  },
  {
    "objectID": "posts/VAE/VAE.html#footnotes",
    "href": "posts/VAE/VAE.html#footnotes",
    "title": "Variational Autoencoders",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI took this example/analogy from a linear alegbra course by Nathan Kutz to describe dimension reduction for Principal Component Analysis. I however, altered and extended the analogy to suit my purposes for explaining VAEs (and ultimately DVAEs and discovering the latent dynamics high dimensional of systems)↩︎"
  },
  {
    "objectID": "posts/Kalman/Kalman.html",
    "href": "posts/Kalman/Kalman.html",
    "title": "Kalman Filters and Generative Modelling",
    "section": "",
    "text": "Kalman filters are a type of Bayesian filter (at least that is the lens we will use to analyze them). They solve a type of mathematical problem known as the filtering problem analytically by assuming we have a linear Gaussian state-space system. This means the relationships between our states and measurements are linear and the states and measurements are assumed to be Normally distributed. Despite these restrictive assumptions Kalman filters are widely used for applications such as object tracking (e.g. missiles, robots, vehicles etc.) and navigation. Even if that wasn’t the case it is fascinating to explore the topic of filtering through the Kalman filter due to how (relatively) accessible it is. In this post I want to go over some of the theory behind the Kalman filter and how Bayesian filters can be used to train generative models. Note, I assume in this post you know about VAEs or DVAEs, so if you don’t, go read my previous posts on those topics or look them up online"
  },
  {
    "objectID": "posts/Kalman/Kalman.html#basic-concept",
    "href": "posts/Kalman/Kalman.html#basic-concept",
    "title": "Kalman Filters and Generative Modelling",
    "section": "Basic Concept",
    "text": "Basic Concept\nImagine we have a tank of liquid, and we want to have an accurate reading of its temperature (perhaps we want to control this variable and keep it at a set value, but that isn’t important here). “Okay, let’s put a sensor that measures the temperature” you say. To which I would reply “good, but can we do better”. Note that the sensors have noise or a certain tolerance, and so we may want to “filter” out this noise and have a tighter tolerance. How could we do this? The Kalman filters answer would be to say that we have more information than just the sensor readings. We have the physics of the system, more precisely we have a mathematical model that can describe how the temperature should change as we add or take away heat from the system. So we can calculate what the temperature should be using the mathematical model, and we have a sensor reading. The Kalman filter is a tool that takes both of these and finds a new distribution that describes an updated temperature distribution which accounts for both the mathematical model’s estimated value and the sensor reading. In a way the Kalman filter is just a fancy way of taking a weighted average between the estimated probability distribution of our mathematical model and the estimated distribution of our sensor value at the current time. The example below shows two Gaussian distributions multiplied to find a new distribution which is essentially a weighted average of the two product Gaussian’s.\n\n\n\n\n\nA few things can be noticed here. One this betrays an important property that is vital to the Kalman filter working. The product of 2 Gaussian distributions is also a Gaussian distribution. Hence, as long as our variables are Normally distributed the Kalman Filtering algorithm will work as it only needs to worry about Gaussian distributions. This is also important because the Gaussian distribution is fully described by the mean and variance of the distribution. Hence, when we are updating to the resultant filtered distribution, we only need to worry about 2 variables, the mean and variance (covariance in the multivariate case). That’s essentially what the Kalman filter is, a set of equations that describe how to propagate the mean and variance forward in time and another set of equations to update the state’s mean and variance based on the mean and variance of the sensor reading (or any observation related to the state). The other important assumption is that these equations have to be linear. This is important for the same reason, if they were nonlinear equations, the distribution would no longer be Gaussian distributions and so the equations would no longer apply.\nNote that other Bayesian filters like the Kalman filter apply the same concept that I explained here. However, they may use non-Gaussian distributions or nonlinear dynamics and so the Kalman filter can no longer be used. Since the Kalman filter is a simple case I am using it in this post to illustrate certain concepts; however, these ideas can be extended to more general filters like the Particle filter."
  },
  {
    "objectID": "posts/Kalman/Kalman.html#algorithm",
    "href": "posts/Kalman/Kalman.html#algorithm",
    "title": "Kalman Filters and Generative Modelling",
    "section": "Algorithm",
    "text": "Algorithm\nGiven the following state space equations for the system,\n\\[\nx_t = F_t x_{t-1} + w_t, \\ w_t \\sim \\mathcal{N}(0,Q_t)\n\\] \\[\ny_t = H_t x_t + v_t, \\ v_t \\sim \\mathcal{N}(0,R_t)\n\\] where, \\(x_t\\) is the state (temperature in our example), \\(y_t\\) is the measurement or observation, \\(F_t\\) describes the dynamics of the state, \\(H_t\\) describes the measurement function that converts states to measurements (e.g. if the measurements are represented by voltages in an electronic sensor \\(H_t\\) converts the state from temperature to voltage), \\(w_t\\) is some random noise added to our mathematical model known as the process noise, \\(Q_t\\) is the covariance of that process noise, \\(v_t\\) is the measurement noise and \\(R_t\\) is the covariance that describes that noise.\nThe equations for the Kalman filter are as follows,\n\nFirst propagate the previous time mean and covariance to the current time using our state space model. Also convert those variables to the measurement domain so we can directly compare our estimates to the observations. If they have different units for example we couldn’t fairly compare them, e.g. it would be like saying 1000mm is way bigger compared to 1m. \\[\n\\hat{x}^-_{t} = F_t \\hat{x}^+_{t-1}\n\\] \\[\nP^-_t = F_t P^+_{t-1} F_t^T + Q_t\n\\] \\[\n\\mu_t = H_t \\hat{x}^-_t\n\\] \\[\nS_t = H_t P_t^- H_t^T + R_k\n\\]\nNow incorporate the measurement (take the weighted average) \\[\nK_t = P^-_t H_t^T S_k^{-1}\n\\] \\[\n\\hat{x}^+_t = x^-_t + K_t(y_t - \\mu_t)\n\\] \\[\nP^+_t = (I - K_t H_t)P^-_t\n\\]\n\nHere I have used the “hat” symbol to refer to the mean of the state i.e. \\(\\hat{x}\\) is the mean of the random variable \\(x_t\\). Also note, \\(P_t\\) is the covariance of the state, the \\(-\\) superscript refers to a “prior” and \\(+\\) is the updated “posterior” statistic e.g. this shows us that \\(P_t^-\\) is the prior covariance before we accounted for the measurement update, and once we do that we get \\(P_t^+\\). \\(\\mu\\) and \\(S\\) are the mean and covariance of the state transfered to the measurement space."
  },
  {
    "objectID": "posts/Kalman/Kalman.html#vaes-and-bayesian-filtering",
    "href": "posts/Kalman/Kalman.html#vaes-and-bayesian-filtering",
    "title": "Kalman Filters and Generative Modelling",
    "section": "VAEs and Bayesian Filtering",
    "text": "VAEs and Bayesian Filtering\nWith a VAE we use Bayes’ theorem to learn a generative model that can generate samples from a data distribution \\(p(Y)\\) (normally we use the variable \\(X\\) but I didn’t want to confuse it with the states from above so since I used \\(y_t\\) for measurements we’re sticking with that). We wanted to optimize the VAE by maximizing \\(p(Y)\\), or more practically, by minimizing \\(-\\text{log}p(Y)\\). But we couldn’t because when using Bayes’ theorem we found,\n\\[\np(X|Y) = \\frac{p(Y|X)p(X)}{p(Y)} = \\frac{p(Y|X)p(X)}{\\int p(Y|X)p(X)dX}\n\\]\nSo \\(p(Y) = \\int p(Y|X)p(X)dX\\) and this was an integral that was too difficult to solve … Wait a minute, isn’t this the marginal likelihood? Yes it is, and the Kalman filter (or a Bayesian filter like the Particle filter) can be used to find this likelihood. This is what those papers I mentioned in the beginning noticed and utilized. They used a Particle filtering approach as Kalman filters restrict you to a Linear Gaussian system (although (Bézenac et al. 2020) uses a Kalman filter with normalizing flows to transform the Gaussian distribution), but the principle remains the same. Also note that instead of a VAE you would extend it to a DVAE (VAE but for sequential data) as Bayesian filters deal with sequential variables. I just used the VAE because the notation is simpler.\nIn the previous section mentioned that an optimization routine could be used along with the marginal likelihood as a loss function to optimize the parameters of the model. I also said backprop could be that optimization routine. A filtering algorithm could therefore be used to train a deep learning model based on the criteria defined by the marginal likelihood. This is exactly what we want a generative model to do, maximize the probability the data we generate (let’s call it \\(\\hat{Y}\\)) comes from the data distribution \\(p(Y)\\). With this sequential case and Bayesian filtering formulation this creates a cool way of seeing how we might learn probabilistic dynamic systems using deep learning.\nEven if you don’t end up using a filtering algorithm to optimize your DVAE and instead use ELBO; I hope this post has shown you why you are secretly learning the state space dynamics of a system when training these models. A lot of the theory behind generative models went over my head when I was first learning about it. But this really helped me ground my understanding in a theory I was comfortable with and understood relatively well. I hope this post managed to help you in a similar way or at least shown you a different perspective on this topic."
  }
]