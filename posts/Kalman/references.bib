@article{Naesseth2017,
   abstract = {Many recent advances in large scale probabilistic inference rely on
variational methods. The success of variational approaches depends on (i)
formulating a flexible parametric family of distributions, and (ii) optimizing
the parameters to find the member of this family that most closely approximates
the exact posterior. In this paper we present a new approximating family of
distributions, the variational sequential Monte Carlo (VSMC) family, and show
how to optimize it in variational inference. VSMC melds variational inference
(VI) and sequential Monte Carlo (SMC), providing practitioners with flexible,
accurate, and powerful Bayesian inference. The VSMC family is a variational
family that can approximate the posterior arbitrarily well, while still
allowing for efficient optimization of its parameters. We demonstrate its
utility on state space models, stochastic volatility models for financial data,
and deep Markov models of brain neural circuits.},
   author = {Christian A. Naesseth and Scott W. Linderman and Rajesh Ranganath and David M. Blei},
   doi = {10.48550/arxiv.1705.11140},
   month = {5},
   title = {Variational Sequential Monte Carlo},
   url = {https://arxiv.org/abs/1705.11140},
   year = {2017},
}
@article{Maddison2017,
   abstract = {When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.},
   author = {Chris J. Maddison and Dieterich Lawson and George Tucker and Nicolas Heess and Mohammad Norouzi and Andriy Mnih and Arnaud Doucet and Yee Whye Teh},
   doi = {10.48550/arxiv.1705.09279},
   month = {5},
   title = {Filtering Variational Objectives},
   url = {http://arxiv.org/abs/1705.09279},
   year = {2017},
}
@inproceedings{Sarkka2013,
  title={Bayesian Filtering and Smoothing},
  author={Simo S{\"a}rkk{\"a}},
  booktitle={Institute of Mathematical Statistics textbooks},
  year={2013}
}
@inproceedings{Bezenac2020,
 author = {de B\'{e}zenac, Emmanuel and Rangapuram, Syama Sundar and Benidis, Konstantinos and Bohlke-Schneider, Michael and Kurle, Richard and Stella, Lorenzo and Hasson, Hilaf and Gallinari, Patrick and Januschowski, Tim},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {2995--3007},
 publisher = {Curran Associates, Inc.},
 title = {Normalizing Kalman Filters for Multivariate Time Series Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1f47cef5e38c952f94c5d61726027439-Paper.pdf},
 volume = {33},
 year = {2020}
}