{"title":"Dynamical Variational Autoencoders","markdown":{"yaml":{"title":"Dynamical Variational Autoencoders","author":"Marco Star","date":"27 June 2022","bibliography":"references.bib","format":{"html":{"code-fold":"show"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n>This post is about the Dynamical Variational Autoencoder, which is like the Variational Autoencoder but for sequences of inputs such as time series. It is meant to be the sequel to my post about Variational Autoencoders\n\nPreviously I have gone over Variational Autoencoders and mentioned that they are not suitable in their common form if we want to learn the underlying latent dynamics of a system. The main problem is that the generative model (the prior and decoder) only generates static data and isn't concerned with the underlying dynamics of the data. For example, if we want to use the VAE to generate a sequence of positions of a moving object $X = [x_1,...,x_T]$ then we would do the following,\n\n* Sample from the prior, $Z = [z_1,...,z_T] \\sim p(Z)$\n* Decode the samples, $p_{\\theta_x}(X|Z)$ and sample generated input data $X$\n\nNote, when I refer to the generative model I refer to the joint distribution between the input variables ($X$) and the latent variables ($Z$) i.e. $p(X,Z)$. This can be written as $p(X,Z) = p(X|Z)p(Z)$ which is just the decoder and prior of a VAE. Hence, when I refer to the generative model I am referring to these two components of the VAE which can practically be applied using the above steps.\n\nHere the generative model essentially defines all possible input position sequences $X=[x_1,...,x_T]$. But what if we want to generate any arbitrary length e.g. we have some data and want to forecast possible future positions until they reach some target position. Or what if I want to understand or use the underlying dynamics behind these positions, so I can generate more accurate or plausible sequences. How would I do this? \n\nLuckily there are a type of VAE called the Dynamical Variational Autoencoder (DVAE) which solves this problem and improves the quality of generated sequential signals when compared to the regular VAE. The same underlying theory from the VAE is still there, in fact the underlying model is pretty much the same we just need to write it in a different mathematical form and we will get the DVAE. I'll try to go through a lot of the basic theory in this post, but if you want a more in-depth resource on DVAEs, then I highly suggest the following review paper [@Girin2020]. \n\n## Theory\n\nSo how do we implement a DVAE? Well lets first look at the components of a regular VAE, \n\n* Inference model: $q_{\\phi}(Z|X)$\n* Generative model: $p_{\\theta}(X,Z) = p_{\\theta_x}(X|Z) p_{\\theta_z}(Z)$ \n\nWhere any subscripts on these probability distributions are the learnable parameters of the neural network that represents the distribution e.g. if $q_{\\phi}(Z|X)$ is the neural network with inputs $X$ and outputs $Z$ then $\\phi$ are the learnable parameters that will be optimised during training. \n\nNote, if you are familiar with VAEs you may wonder why the prior $p_{\\theta_z}(Z)$ has parameters $\\theta_z$ associated with it, as we normally define it as a standard normal distribution (so there is nothing to optimise). This is because in the DVAE formulation it will be a neural network, so it's a teaser for later (you can ignore it for now).\n\nNow all we need to do is say that these variables are sequences of some arbitrary length $T$ i.e. $X= x_{1:T} = [x_1,...,x_T]$ and $Z = z_{1:T} =[z_1,...,z_T]$ and substitute this in. \n\n* Inference model: $q_{\\phi}(z_{1:T}|x_{1:T})$\n* Generative model: $p_{\\theta}(x_{1:T},z_{1:T}) = p_{\\theta_x}(x_{1:T}|z_{1:T}) p_{\\theta_z}(z_{1:T})$ \n\nOkay, you might be thinking \"how has this helped\", well we can now rewrite these probability distributions as products of other probability distributions. Let's first illustrate this for the generative model.\n\n$$\n    \\text{Generative: } p_{\\theta}(x_{1:T}, z_{1:T}) = \\prod_{t=1}^T p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}) p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\n$$\n\nNotice now we have this causal model, which is to say variables at time $t$ only depend on past variables $1:t-1$ (the present value is determined by the past values). This means our new decoder and prior models become, \n\n$$\n    \\text{Decoder: } p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}) \\\\\n    \\text{Prior: } p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\n$$\n\nThe prior generates the next latent variable $z_t$ which can then be used to input all $z_{1:t}$ into the decoder. We can alternate or chain this operation to generate an entire sequence of $x_{1:T}$ for any arbitrary value of $T$ by simply alternating between generating the next $z_t$ value and then generate $x_t$, which helps generate $z_{t+1}$ which generates $x_{t+1}$ and so on. \n\nBut to train the generative model we need an inference model to encode the inputs to a latent space. When breaking the inference model into a product of distributions we get, \n\n$$\n    \\text{Inference: } q_{\\phi}(z_{1:T}|x_{1:T}) = \\prod_{t=1}^T q_{\\phi}(z_t|z_{1:t-1}, x_{1:T}) \n$$\n\nThis cannot be simplified further; unlike the joint distribution we are stuck with the fact we are given $x_{1:T}$. Hence, this is a noncausal model where future values are needed to calculate present variables at time $t$. Since the inference model is only used when training, $x_{1:T}$ is anyway a given/available to us. This is because it is our training data, so it's not a big deal that this model is noncausal (we are not cheating by looking into the future). But this does have an important impact on the model which may not be intuitive. As my gut instinct if I had to extend the VAE for sequential inputs would be to use $x_{1:t}$ for the inference model. However, by breaking the probability distribution of the sequence into the product of distributions which represent the current values at time $t$ we can see $x_{1:T}$ is needed for the inference model. But how do we represent entire sequences (which could be any arbitrary length $T$) so that they can be used as inputs to a neural network?\n\nThe most common way is to use a Recurrent Neural Network (RNN). Many implementations will input a sequence (e.g. $x_{1:t}$) and let the hidden variable at that time $t$ be a representation of the entire sequence i.e. $h_t := x_{1:t}$. This can be seen in the diagram below where the RNN cell takes in the previous hidden state ($h_{t-1}$) and current input value ($x_t$) and outputs $h_t$. Since each hidden state carries the information of the previous and current input, $x_{1:t}$, the hidden state $h_t$ is used to represent this input sequence. Note theoretically other models could be used to represent the input sequence $x_{1:t}$, but the RNN is chosen due to its simplicity as it represents the entire sequence with just one variable $h_t$. For $x_{1:T}$ we can use a forward-backward RNN, where we essentially run the forward RNN (as shown in the diagram) and then run another backwards (from $t=T$ to $t=1$) with $h_{T:1}$ and $x_{T:1}$ as the inputs to the RNN (we can concatenate them together in practice, so they are one input). We will use $\\overleftarrow{h}_t$ as the symbol for hidden variable output of a forward-backwards RNN where, $\\overleftarrow{h}_t := x_{1:T}$. \n\n![](images/DVAE_RNN_rep.png)\n\nWe now need to construct a network which will represent the inference distribution $q_{\\phi}(z_t|z_{1:t-1}, x_{1:T})$ and the generative model distributions, i.e. the prior $p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})$ and the decoder $p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t})$. This can be done by looking at the given values of each distribution and using those variables as the inputs to a neural network. The output of the neural network can be the mean and variance of a Gaussian distribution for simplicity. Hence, could construct the following networks as an attempt to model these distributions, \n\n$$\n    \\text{inference network: } f_{\\phi}(z_{1:t-1}, x_{1:T}) = f_{\\phi}(z_{t-1}, \\overleftarrow{h}_t) = [\\mu_{\\hat{z}}, \\sigma_{\\hat{z}}] \n$$\n$$\n    \\text{prior network: } f_{\\theta_z}(z_{1:t-1}, x_{1:t-1}) = f_{\\theta_z}(z_{t-1}, h_{t}) = [\\mu_{z}, \\sigma_{z}] \n$$\n$$\n    \\text{decoder network: } f_{\\theta_x}(x_{1:t-1}, z_{1:t}) = f_{\\theta_x}(h_t, z_t) = [\\mu_{x}, \\sigma_{x}]\n$$\n\nWhere each network outputs a mean $\\mu$ and standard deviation $\\sigma$ to define a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma)$, also note how each data sequence ($x_{1:t-1}$) was replaced by a hidden variable representation using a RNN ($x_{1:t-1} := h_{t}$). We also assumed for simplicity that the latent dynamics followed a Markov assumption so $z_t|z_{1:t-1}$ became $z_t|z_{t-1}$ i.e. the current variable only depends on the previous time variable. It should be mentioned that from a practical perspective the networks do not usually output the standard deviation but the log-variance ($\\text{log}\\sigma^2$) as the range is not restricted to only positive values, note you can make the standard deviation an output as long as an activation function is used to ensure the value is positive (e.g. softplus). \n\nAnother way of representing these equations and designing DVAEs is by using block diagrams. For example, you can see one of the inference model below. \n\n![](images/DVAE_inf_ex.png)\n\nWith this diagram we can see how the data is handled. Each variable inside a circle is a stochastic variable, hence, if an arrow points to a circle we know that the output of the network that outputs that variable would be $(\\mu, \\sigma)$ for our example as we are using Gaussian distributions to model random variables. Rectangle blocks are deterministic variables and can therefore be direct outputs of a neural network (all the deterministic variables in the diagram are hidden variable outputs from a RNN). The diagram can make certain concepts clearer e.g. we can see why $h_t := x_{1:t-1}$ as each $h_t$ is generated from inputs $x_{t-1}$ and $h_{t-1}$, since $h_{t-1}:=x_{1:t-2}$, $h_t$ is a union of $x_{1:t-2}$ and $x_{t-1}$ which means it represents $x_{1:t-1}$. \n\nThe diagram below is for the generative model. \n\n![](images/DVAE_gen_ex.png)\n\nNotice that the generative model ultimately outputs $x_t$ variables using the latent dynamics of $z_t$. We can see this from the diagram through the arrows which point to $x_t$ from $h_t$ and $z_t$ (meaning $h_t$ and $z_t$ are inputs). Since, $h_t := x_{1:t-1}$ we have $f_{\\theta_x}(x_{1:t-1},z_t) = x_t$ which is what we stated before as the decoder model. Hence, these diagrams can be a handy visual representation of the equations we stated previously, and sometimes I find it useful to draw the diagrams as they help me see how the data \"flows\" through the networks, and then from the diagrams see what my equations and probability distribution representations are. \n\nFinally, we should look at the loss function used to train the DVAE. Let us first have a look the loss function for a regular VAE i.e. the Evidence Lower Bound (ELBO),\n\n$$\n\\text{ELBO} = \\mathbb{E}_{q_{\\phi}(Z|X)}\\left[\\text{log} p_{\\theta_x}(X|Z)\\right] - D_{KL}\\left( q_{\\phi}(Z|X) || p(Z)\\right)\n$$\n\nWe can do what we did previously to define the DVAE from the VAE formulation, that is, make the following replacements, $X= x_{1:T} = [x_1,...,x_T]$ and $Z = z_{1:T} =[z_1,...,z_T]$ resulting in,\n\n$$\n\\text{ELBO} = \\mathbb{E}_{q_{\\phi}(z_{1:T}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{1:T}|z_{1:T})\\right] - D_{KL}\\left( q_{\\phi}(z_{1:T}|x_{1:T}) || p_{\\theta_z}(z_{1:T})\\right)\n$$\n\nFrom the previous analysis we know we can express the distribution of a sequence such as $x_{1:T}$ with a product of distributions defining $x_t$ given the previous $x_{1:t-1}$. Since both loss terms have $log$ acting on the distributions, the product over time period $T$ can become a sum over $T$. The final loss ends up being, \n\n$$\n\\text{ELBO} = \\sum_{t=1}^{T} \\mathbb{E}_{q_{\\phi}(z_{1:t}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{t}|x_{1:t-1}, z_{1:t})\\right] - \\sum_{t=1}^T \\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})} \\left[D_{KL}\\left( q_{\\phi}(z_{t}|z_{1:t-1}, x_{1:T}) || p_{\\theta_z}(z_{t}| z_{1:t-1}, x_{1:t-1})\\right)\\right]\n$$\n\nNote the expected value is with respect to the inference distribution. Hence, when you see a term like $\\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})}$ is the loss function above, this means the sequence $z_{1:t-1}$ should come from the inference model. \nFor example, the KL-divergence has the term, $p_{\\theta_z}(z_{t}| z_{1:t-1}, x_{1:t-1})$, inside the expection $\\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})}$. This means we use the prior model to generate $z_t$ but the previous latent variables $z_{1:t-1}$ came from the inference model. \nAnother example is the log-likelihood which is $\\mathbb{E}_{q_{\\phi}(z_{1:t}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{1:t}|x_{1:t-1}, z_{1:t})\\right]$. Here we use the decoder $p_{\\theta_x}$ to generate $x_t$ using the previous $x_{1:t-1}$ values and the latent variables again from the inference model as the expectation ($\\mathbb{E}$) is with respect to the inference distribution $q_{\\phi}(z_{1:t}|x_{1:T})$.\nAgain as a reminder, for a more detailed look at all this theory {% cite Girin2020 %} has an awesome review paper on the topic. \n\n## Constructing a DVAE \n\nNow lets look at an example of using the DVAE to model a dynamic system. I used the example of a mass-spring system in my post about VAEs, but stated we needed a way of handling sequences to allow the network to find the underlying dynamics of the system. Now that we have the DVAE in our tool kit, we have a generative model well suited to handle sequence inputs. As a reminder the diagram of the mass-spring system is below, where the mass is moving up and down in the z-direction. The idea here is that we pretend we have never seen a mass-spring system before in our lives, and so we set up sensors around the room to measure the distance between the sensor and the mass (these are labelled as $x$ on the diagram). If we knew to set up the sensor in the z-direction we would have a nice 1D representation of the system dynamics, but we don't know that, so instead we have a bunch of sensor signals which together give us a $n$-dimentional representation of the system dynamics. Our aim is to use the DVAE to find a lower dimensional representation of the system (it does not necessarily have to be the dynamics in the z-direction). \n\n![](images/mass_spring_VAE.png)\n\nWe can generate some of these sensor readings by simulating the motion of the system and placing $n$ sensors randomly around a room by specifying their Cartesian coordinates,\n\n$$\n    (x, y, z) \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n$$\n\nHere $n=14$ and we can plot what each sensor looks like as an example of the dynamics of 1 mass-spring system. \n\nTo train a DVAE we will need a whole dataset of mass-spring system dynamics. Hence, I will simulate 100 different mass-spring systems with slightly different mass, spring constant and initial values for training and another 100 for testing the model. From now on I will refer to each different mass-spring system as a \"unit\" (as its easier to type). Hence, we have 100 different training units and 100 testing units. For model training, I have also used a sliding window so inputs into the model are \"packets\" of the signal with a time window size $T=40$. So inputs are tensors of size (batch size, T, sensors) while the testing inputs are still the entire time sequence. The training data is split into these time windowed packets, simply so we have more data that can be computed in parallel which allows for quicker training on a GPU. Each sensor signal is also normalised to have a mean of zero and a standard deviation of one. This helps with training the network as the signals don't have varying magnitudes, so the network can stick to a known range. \n\nHere I simply have some code that defines some utilities like a simple Feedforward or Multilayer Perceptron (MLP) neural network as well as some loss functions needed in the DVAE i.e. the KL divergence between two Gaussian distributions and the log-likelihood of a Gaussian distribution. \n\nThe following is code for the actual DVAE itself.\n\nNow that we have defined all the components we can train the model.\n\nHere we will test the DVAE by first seeing if the generative model can simply reconstruct the sensor signals given. Essentially we will give it the true $x_{t-1}$ value and it will generate $z_t$, which will be decoded to $x_t$. This will be done for all $t \\in [0,T]$ where $T$ is the entire sequence (instead of just a time window like in training). Since $x_0$ is the estimated I will ignore it when plotting as the DVAE did not generate this value. \n\nI only plot one sensor below otherwise I find it gets to cluttered and difficult to see how well it performed. \n\nWe can also plot the latent variable sequences we generated when we reconstructed all these sensors. \n\nPlotting all the latent variable sequences we get. \n\nYou could imagine we could interpolate a new latent sequence in this latent space using our own initial condition and the prior latent dynamics. If we did this and decoded it back to the input space we should get new reconstructed sensor values of some fictitious mass-spring system that doesn't exist (I'm aware none of these systems existed as I simulated them, but if this was all done through real life experiments I could simulate a new realistic looking virtual experiment). This could be applied in interesting applications such as generating audio, where we use real audio signals to train the model and then generate new audio using the trained model. We can also use a conditional DVAE where we generate new inputs conditioned on other variables. This can be used in machinery prognostics where you might want to find the Remaining Useful Life (RUL) of a machine or component. The RUL could be the generated inputs and these can be conditioned on sensor values. This would change the DVAE model to, \n\n$$\n    \\text{Encoder/Inference: } q_{\\phi}(z_t|z_{1:t-1}, x_{1:T}, u_{1:T}) \\\\\n    \\text{Decoder: } p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}, u_{1:t}) \\\\\n    \\text{Prior: } p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1}, u_{1:t})\n$$\n\nwhere in this example $x_t = (RUL)_t$ and $u_t = $ sensor signals at time $t$. \n\nThe code below shows how one would generate new signals from \"virtual\" mass-spring systems that were not seen during training. \n\nHere we give our model 70% of the sensors and ask it to extrapolate the 30% we didn't give it. We can plot the results below.\n\nThis doesn't look too bad, but each time we are sampling a different $z_t$ value at each time and decoding it. Hence, each time we call the function to extrapolate we generate a different trajectory $z_{1:T}$ and decode to $x_{1:T}$. So lets run the same code again and get an idea of how much the extrapolated results can vary. \n\nAs you can see the trajectory still maintains the oscillating dynamics but can have a lot of variation when extrapolating. But we can generate signals which look somewhat like the original signal in the sense certain characteristics (such as the oscillations) are captured by the underlying prior latent dynamics. The latent dynamics could be smoother if we played around with the prior model. Here we just used a standard feedforward network which took $z_{t-1}$ to $z_{t}$. To improve the results perhaps a better prior model could be used with more structured dynamics. But hopefully the theory and the simple example (relatively speaking) helps you if you want to use these for an application. So we at least finish this post with an idea of how to improve the model and a cool way of generating dynamic data. \n\nI think this is a good place to stop. Ultimately the aim of these posts is to build up to the point where we are using DVAEs to estimate the RUL of machinery based on input sensor signals. So the theory and basic examples shown here is to get everyone comfortable with the basic ideas behind DVAEs before we move on to more practical applications. Hopefully this post has aided your understanding of DVAEs (and/or introduced you to a cool new technique), I know the mass-spring system analogy really helped me think about how DVAEs are VAEs but extended to deal with dynamic systems or sequential data. \n","srcMarkdownNoYaml":"\n\n>This post is about the Dynamical Variational Autoencoder, which is like the Variational Autoencoder but for sequences of inputs such as time series. It is meant to be the sequel to my post about Variational Autoencoders\n\n## Introduction\nPreviously I have gone over Variational Autoencoders and mentioned that they are not suitable in their common form if we want to learn the underlying latent dynamics of a system. The main problem is that the generative model (the prior and decoder) only generates static data and isn't concerned with the underlying dynamics of the data. For example, if we want to use the VAE to generate a sequence of positions of a moving object $X = [x_1,...,x_T]$ then we would do the following,\n\n* Sample from the prior, $Z = [z_1,...,z_T] \\sim p(Z)$\n* Decode the samples, $p_{\\theta_x}(X|Z)$ and sample generated input data $X$\n\nNote, when I refer to the generative model I refer to the joint distribution between the input variables ($X$) and the latent variables ($Z$) i.e. $p(X,Z)$. This can be written as $p(X,Z) = p(X|Z)p(Z)$ which is just the decoder and prior of a VAE. Hence, when I refer to the generative model I am referring to these two components of the VAE which can practically be applied using the above steps.\n\nHere the generative model essentially defines all possible input position sequences $X=[x_1,...,x_T]$. But what if we want to generate any arbitrary length e.g. we have some data and want to forecast possible future positions until they reach some target position. Or what if I want to understand or use the underlying dynamics behind these positions, so I can generate more accurate or plausible sequences. How would I do this? \n\nLuckily there are a type of VAE called the Dynamical Variational Autoencoder (DVAE) which solves this problem and improves the quality of generated sequential signals when compared to the regular VAE. The same underlying theory from the VAE is still there, in fact the underlying model is pretty much the same we just need to write it in a different mathematical form and we will get the DVAE. I'll try to go through a lot of the basic theory in this post, but if you want a more in-depth resource on DVAEs, then I highly suggest the following review paper [@Girin2020]. \n\n## Theory\n\nSo how do we implement a DVAE? Well lets first look at the components of a regular VAE, \n\n* Inference model: $q_{\\phi}(Z|X)$\n* Generative model: $p_{\\theta}(X,Z) = p_{\\theta_x}(X|Z) p_{\\theta_z}(Z)$ \n\nWhere any subscripts on these probability distributions are the learnable parameters of the neural network that represents the distribution e.g. if $q_{\\phi}(Z|X)$ is the neural network with inputs $X$ and outputs $Z$ then $\\phi$ are the learnable parameters that will be optimised during training. \n\nNote, if you are familiar with VAEs you may wonder why the prior $p_{\\theta_z}(Z)$ has parameters $\\theta_z$ associated with it, as we normally define it as a standard normal distribution (so there is nothing to optimise). This is because in the DVAE formulation it will be a neural network, so it's a teaser for later (you can ignore it for now).\n\nNow all we need to do is say that these variables are sequences of some arbitrary length $T$ i.e. $X= x_{1:T} = [x_1,...,x_T]$ and $Z = z_{1:T} =[z_1,...,z_T]$ and substitute this in. \n\n* Inference model: $q_{\\phi}(z_{1:T}|x_{1:T})$\n* Generative model: $p_{\\theta}(x_{1:T},z_{1:T}) = p_{\\theta_x}(x_{1:T}|z_{1:T}) p_{\\theta_z}(z_{1:T})$ \n\nOkay, you might be thinking \"how has this helped\", well we can now rewrite these probability distributions as products of other probability distributions. Let's first illustrate this for the generative model.\n\n$$\n    \\text{Generative: } p_{\\theta}(x_{1:T}, z_{1:T}) = \\prod_{t=1}^T p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}) p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\n$$\n\nNotice now we have this causal model, which is to say variables at time $t$ only depend on past variables $1:t-1$ (the present value is determined by the past values). This means our new decoder and prior models become, \n\n$$\n    \\text{Decoder: } p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}) \\\\\n    \\text{Prior: } p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})\n$$\n\nThe prior generates the next latent variable $z_t$ which can then be used to input all $z_{1:t}$ into the decoder. We can alternate or chain this operation to generate an entire sequence of $x_{1:T}$ for any arbitrary value of $T$ by simply alternating between generating the next $z_t$ value and then generate $x_t$, which helps generate $z_{t+1}$ which generates $x_{t+1}$ and so on. \n\nBut to train the generative model we need an inference model to encode the inputs to a latent space. When breaking the inference model into a product of distributions we get, \n\n$$\n    \\text{Inference: } q_{\\phi}(z_{1:T}|x_{1:T}) = \\prod_{t=1}^T q_{\\phi}(z_t|z_{1:t-1}, x_{1:T}) \n$$\n\nThis cannot be simplified further; unlike the joint distribution we are stuck with the fact we are given $x_{1:T}$. Hence, this is a noncausal model where future values are needed to calculate present variables at time $t$. Since the inference model is only used when training, $x_{1:T}$ is anyway a given/available to us. This is because it is our training data, so it's not a big deal that this model is noncausal (we are not cheating by looking into the future). But this does have an important impact on the model which may not be intuitive. As my gut instinct if I had to extend the VAE for sequential inputs would be to use $x_{1:t}$ for the inference model. However, by breaking the probability distribution of the sequence into the product of distributions which represent the current values at time $t$ we can see $x_{1:T}$ is needed for the inference model. But how do we represent entire sequences (which could be any arbitrary length $T$) so that they can be used as inputs to a neural network?\n\nThe most common way is to use a Recurrent Neural Network (RNN). Many implementations will input a sequence (e.g. $x_{1:t}$) and let the hidden variable at that time $t$ be a representation of the entire sequence i.e. $h_t := x_{1:t}$. This can be seen in the diagram below where the RNN cell takes in the previous hidden state ($h_{t-1}$) and current input value ($x_t$) and outputs $h_t$. Since each hidden state carries the information of the previous and current input, $x_{1:t}$, the hidden state $h_t$ is used to represent this input sequence. Note theoretically other models could be used to represent the input sequence $x_{1:t}$, but the RNN is chosen due to its simplicity as it represents the entire sequence with just one variable $h_t$. For $x_{1:T}$ we can use a forward-backward RNN, where we essentially run the forward RNN (as shown in the diagram) and then run another backwards (from $t=T$ to $t=1$) with $h_{T:1}$ and $x_{T:1}$ as the inputs to the RNN (we can concatenate them together in practice, so they are one input). We will use $\\overleftarrow{h}_t$ as the symbol for hidden variable output of a forward-backwards RNN where, $\\overleftarrow{h}_t := x_{1:T}$. \n\n![](images/DVAE_RNN_rep.png)\n\nWe now need to construct a network which will represent the inference distribution $q_{\\phi}(z_t|z_{1:t-1}, x_{1:T})$ and the generative model distributions, i.e. the prior $p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1})$ and the decoder $p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t})$. This can be done by looking at the given values of each distribution and using those variables as the inputs to a neural network. The output of the neural network can be the mean and variance of a Gaussian distribution for simplicity. Hence, could construct the following networks as an attempt to model these distributions, \n\n$$\n    \\text{inference network: } f_{\\phi}(z_{1:t-1}, x_{1:T}) = f_{\\phi}(z_{t-1}, \\overleftarrow{h}_t) = [\\mu_{\\hat{z}}, \\sigma_{\\hat{z}}] \n$$\n$$\n    \\text{prior network: } f_{\\theta_z}(z_{1:t-1}, x_{1:t-1}) = f_{\\theta_z}(z_{t-1}, h_{t}) = [\\mu_{z}, \\sigma_{z}] \n$$\n$$\n    \\text{decoder network: } f_{\\theta_x}(x_{1:t-1}, z_{1:t}) = f_{\\theta_x}(h_t, z_t) = [\\mu_{x}, \\sigma_{x}]\n$$\n\nWhere each network outputs a mean $\\mu$ and standard deviation $\\sigma$ to define a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma)$, also note how each data sequence ($x_{1:t-1}$) was replaced by a hidden variable representation using a RNN ($x_{1:t-1} := h_{t}$). We also assumed for simplicity that the latent dynamics followed a Markov assumption so $z_t|z_{1:t-1}$ became $z_t|z_{t-1}$ i.e. the current variable only depends on the previous time variable. It should be mentioned that from a practical perspective the networks do not usually output the standard deviation but the log-variance ($\\text{log}\\sigma^2$) as the range is not restricted to only positive values, note you can make the standard deviation an output as long as an activation function is used to ensure the value is positive (e.g. softplus). \n\nAnother way of representing these equations and designing DVAEs is by using block diagrams. For example, you can see one of the inference model below. \n\n![](images/DVAE_inf_ex.png)\n\nWith this diagram we can see how the data is handled. Each variable inside a circle is a stochastic variable, hence, if an arrow points to a circle we know that the output of the network that outputs that variable would be $(\\mu, \\sigma)$ for our example as we are using Gaussian distributions to model random variables. Rectangle blocks are deterministic variables and can therefore be direct outputs of a neural network (all the deterministic variables in the diagram are hidden variable outputs from a RNN). The diagram can make certain concepts clearer e.g. we can see why $h_t := x_{1:t-1}$ as each $h_t$ is generated from inputs $x_{t-1}$ and $h_{t-1}$, since $h_{t-1}:=x_{1:t-2}$, $h_t$ is a union of $x_{1:t-2}$ and $x_{t-1}$ which means it represents $x_{1:t-1}$. \n\nThe diagram below is for the generative model. \n\n![](images/DVAE_gen_ex.png)\n\nNotice that the generative model ultimately outputs $x_t$ variables using the latent dynamics of $z_t$. We can see this from the diagram through the arrows which point to $x_t$ from $h_t$ and $z_t$ (meaning $h_t$ and $z_t$ are inputs). Since, $h_t := x_{1:t-1}$ we have $f_{\\theta_x}(x_{1:t-1},z_t) = x_t$ which is what we stated before as the decoder model. Hence, these diagrams can be a handy visual representation of the equations we stated previously, and sometimes I find it useful to draw the diagrams as they help me see how the data \"flows\" through the networks, and then from the diagrams see what my equations and probability distribution representations are. \n\nFinally, we should look at the loss function used to train the DVAE. Let us first have a look the loss function for a regular VAE i.e. the Evidence Lower Bound (ELBO),\n\n$$\n\\text{ELBO} = \\mathbb{E}_{q_{\\phi}(Z|X)}\\left[\\text{log} p_{\\theta_x}(X|Z)\\right] - D_{KL}\\left( q_{\\phi}(Z|X) || p(Z)\\right)\n$$\n\nWe can do what we did previously to define the DVAE from the VAE formulation, that is, make the following replacements, $X= x_{1:T} = [x_1,...,x_T]$ and $Z = z_{1:T} =[z_1,...,z_T]$ resulting in,\n\n$$\n\\text{ELBO} = \\mathbb{E}_{q_{\\phi}(z_{1:T}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{1:T}|z_{1:T})\\right] - D_{KL}\\left( q_{\\phi}(z_{1:T}|x_{1:T}) || p_{\\theta_z}(z_{1:T})\\right)\n$$\n\nFrom the previous analysis we know we can express the distribution of a sequence such as $x_{1:T}$ with a product of distributions defining $x_t$ given the previous $x_{1:t-1}$. Since both loss terms have $log$ acting on the distributions, the product over time period $T$ can become a sum over $T$. The final loss ends up being, \n\n$$\n\\text{ELBO} = \\sum_{t=1}^{T} \\mathbb{E}_{q_{\\phi}(z_{1:t}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{t}|x_{1:t-1}, z_{1:t})\\right] - \\sum_{t=1}^T \\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})} \\left[D_{KL}\\left( q_{\\phi}(z_{t}|z_{1:t-1}, x_{1:T}) || p_{\\theta_z}(z_{t}| z_{1:t-1}, x_{1:t-1})\\right)\\right]\n$$\n\nNote the expected value is with respect to the inference distribution. Hence, when you see a term like $\\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})}$ is the loss function above, this means the sequence $z_{1:t-1}$ should come from the inference model. \nFor example, the KL-divergence has the term, $p_{\\theta_z}(z_{t}| z_{1:t-1}, x_{1:t-1})$, inside the expection $\\mathbb{E}_{q_{\\phi}(z_{1:t-1}|x_{1:T})}$. This means we use the prior model to generate $z_t$ but the previous latent variables $z_{1:t-1}$ came from the inference model. \nAnother example is the log-likelihood which is $\\mathbb{E}_{q_{\\phi}(z_{1:t}|x_{1:T})}\\left[\\text{log} p_{\\theta_x}(x_{1:t}|x_{1:t-1}, z_{1:t})\\right]$. Here we use the decoder $p_{\\theta_x}$ to generate $x_t$ using the previous $x_{1:t-1}$ values and the latent variables again from the inference model as the expectation ($\\mathbb{E}$) is with respect to the inference distribution $q_{\\phi}(z_{1:t}|x_{1:T})$.\nAgain as a reminder, for a more detailed look at all this theory {% cite Girin2020 %} has an awesome review paper on the topic. \n\n## Constructing a DVAE \n\nNow lets look at an example of using the DVAE to model a dynamic system. I used the example of a mass-spring system in my post about VAEs, but stated we needed a way of handling sequences to allow the network to find the underlying dynamics of the system. Now that we have the DVAE in our tool kit, we have a generative model well suited to handle sequence inputs. As a reminder the diagram of the mass-spring system is below, where the mass is moving up and down in the z-direction. The idea here is that we pretend we have never seen a mass-spring system before in our lives, and so we set up sensors around the room to measure the distance between the sensor and the mass (these are labelled as $x$ on the diagram). If we knew to set up the sensor in the z-direction we would have a nice 1D representation of the system dynamics, but we don't know that, so instead we have a bunch of sensor signals which together give us a $n$-dimentional representation of the system dynamics. Our aim is to use the DVAE to find a lower dimensional representation of the system (it does not necessarily have to be the dynamics in the z-direction). \n\n![](images/mass_spring_VAE.png)\n\nWe can generate some of these sensor readings by simulating the motion of the system and placing $n$ sensors randomly around a room by specifying their Cartesian coordinates,\n\n$$\n    (x, y, z) \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n$$\n\nHere $n=14$ and we can plot what each sensor looks like as an example of the dynamics of 1 mass-spring system. \n\nTo train a DVAE we will need a whole dataset of mass-spring system dynamics. Hence, I will simulate 100 different mass-spring systems with slightly different mass, spring constant and initial values for training and another 100 for testing the model. From now on I will refer to each different mass-spring system as a \"unit\" (as its easier to type). Hence, we have 100 different training units and 100 testing units. For model training, I have also used a sliding window so inputs into the model are \"packets\" of the signal with a time window size $T=40$. So inputs are tensors of size (batch size, T, sensors) while the testing inputs are still the entire time sequence. The training data is split into these time windowed packets, simply so we have more data that can be computed in parallel which allows for quicker training on a GPU. Each sensor signal is also normalised to have a mean of zero and a standard deviation of one. This helps with training the network as the signals don't have varying magnitudes, so the network can stick to a known range. \n\nHere I simply have some code that defines some utilities like a simple Feedforward or Multilayer Perceptron (MLP) neural network as well as some loss functions needed in the DVAE i.e. the KL divergence between two Gaussian distributions and the log-likelihood of a Gaussian distribution. \n\nThe following is code for the actual DVAE itself.\n\nNow that we have defined all the components we can train the model.\n\nHere we will test the DVAE by first seeing if the generative model can simply reconstruct the sensor signals given. Essentially we will give it the true $x_{t-1}$ value and it will generate $z_t$, which will be decoded to $x_t$. This will be done for all $t \\in [0,T]$ where $T$ is the entire sequence (instead of just a time window like in training). Since $x_0$ is the estimated I will ignore it when plotting as the DVAE did not generate this value. \n\nI only plot one sensor below otherwise I find it gets to cluttered and difficult to see how well it performed. \n\nWe can also plot the latent variable sequences we generated when we reconstructed all these sensors. \n\nPlotting all the latent variable sequences we get. \n\nYou could imagine we could interpolate a new latent sequence in this latent space using our own initial condition and the prior latent dynamics. If we did this and decoded it back to the input space we should get new reconstructed sensor values of some fictitious mass-spring system that doesn't exist (I'm aware none of these systems existed as I simulated them, but if this was all done through real life experiments I could simulate a new realistic looking virtual experiment). This could be applied in interesting applications such as generating audio, where we use real audio signals to train the model and then generate new audio using the trained model. We can also use a conditional DVAE where we generate new inputs conditioned on other variables. This can be used in machinery prognostics where you might want to find the Remaining Useful Life (RUL) of a machine or component. The RUL could be the generated inputs and these can be conditioned on sensor values. This would change the DVAE model to, \n\n$$\n    \\text{Encoder/Inference: } q_{\\phi}(z_t|z_{1:t-1}, x_{1:T}, u_{1:T}) \\\\\n    \\text{Decoder: } p_{\\theta_x}(x_t|x_{1:t-1}, z_{1:t}, u_{1:t}) \\\\\n    \\text{Prior: } p_{\\theta_z}(z_t|z_{1:t-1}, x_{1:t-1}, u_{1:t})\n$$\n\nwhere in this example $x_t = (RUL)_t$ and $u_t = $ sensor signals at time $t$. \n\nThe code below shows how one would generate new signals from \"virtual\" mass-spring systems that were not seen during training. \n\nHere we give our model 70% of the sensors and ask it to extrapolate the 30% we didn't give it. We can plot the results below.\n\nThis doesn't look too bad, but each time we are sampling a different $z_t$ value at each time and decoding it. Hence, each time we call the function to extrapolate we generate a different trajectory $z_{1:T}$ and decode to $x_{1:T}$. So lets run the same code again and get an idea of how much the extrapolated results can vary. \n\nAs you can see the trajectory still maintains the oscillating dynamics but can have a lot of variation when extrapolating. But we can generate signals which look somewhat like the original signal in the sense certain characteristics (such as the oscillations) are captured by the underlying prior latent dynamics. The latent dynamics could be smoother if we played around with the prior model. Here we just used a standard feedforward network which took $z_{t-1}$ to $z_{t}$. To improve the results perhaps a better prior model could be used with more structured dynamics. But hopefully the theory and the simple example (relatively speaking) helps you if you want to use these for an application. So we at least finish this post with an idea of how to improve the model and a cool way of generating dynamic data. \n\nI think this is a good place to stop. Ultimately the aim of these posts is to build up to the point where we are using DVAEs to estimate the RUL of machinery based on input sensor signals. So the theory and basic examples shown here is to get everyone comfortable with the basic ideas behind DVAEs before we move on to more practical applications. Hopefully this post has aided your understanding of DVAEs (and/or introduced you to a cool new technique), I know the mass-spring system analogy really helped me think about how DVAEs are VAEs but extended to deal with dynamic systems or sequential data. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"DVAE.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.310","theme":"pulse","title-block-banner":true,"title":"Dynamical Variational Autoencoders","author":"Marco Star","date":"27 June 2022","bibliography":["references.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}